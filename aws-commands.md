
# Commandes AWS utiles pour EMR et IAM

Toutes les commandes AWS CLI document√©es ici sont √† ex√©cuter dans un terminal o√π l‚ÄôAWS CLI est install√©e et configur√©e avec des identifiants ayant les droits n√©cessaires (IAM user avec permissions EMR, S3, IAM selon les besoins).

---

## Upload du dataset vers S3

Pour transf√©rer ton dataset local vers le bucket S3 (exemple : `oc-p11-fruits-david-scanu`), tu peux utiliser la commande suivante‚ÄØ:

```bash
aws s3 cp --recursive ./data/raw/fruits-360_dataset/fruits-360/Training/ \
	s3://oc-p11-fruits-david-scanu/data/raw/Training/
```

Cette commande copie tout le dossier `Training` (et son contenu) dans le bucket S3, en conservant l‚Äôarborescence.

Pour v√©rifier l‚Äôupload‚ÄØ:

```bash
aws s3 ls s3://oc-p11-fruits-david-scanu/data/raw/Training/Apple\ Braeburn/
```

**Remarque**‚ÄØ: Les espaces dans les noms de dossiers/fichiers doivent √™tre √©chapp√©s avec un antislash (`\`) ou entour√©s de guillemets.

---

## Autorisations n√©cessaires pour EMR_EC2_DefaultRole (R√©sum√©)

Le r√¥le `EMR_EC2_DefaultRole` doit avoir les autorisations suivantes pour un fonctionnement optimal du cluster EMR‚ÄØ:

- **Acc√®s S3**‚ÄØ: lecture/√©criture sur les buckets utilis√©s (logs, scripts, donn√©es, persistance Jupyter).
- **Policy g√©r√©e AmazonElasticMapReduceforEC2Role**‚ÄØ: donne acc√®s √† EC2, CloudWatch, S3 (certains usages), DynamoDB, KMS, etc. (d√©j√† attach√©e par d√©faut).
- **CloudWatch Logs**‚ÄØ: pour √©crire les logs Spark/Hadoop/Jupyter (inclus dans la policy g√©r√©e).
- **KMS**‚ÄØ: si tu utilises des buckets S3 chiffr√©s avec KMS.
- **DynamoDB**‚ÄØ: si tu utilises EMRFS consistent view.
- **Glue**‚ÄØ: si tu utilises le Data Catalog Glue comme metastore Hive/Spark.
- **Autres (optionnel)**‚ÄØ: `CloudWatchAgentServerPolicy` (monitoring avanc√©), `AmazonSSMManagedInstanceCore` (acc√®s SSM/Session Manager).

Pour la majorit√© des cas EMR Spark/Hadoop/Jupyter, la policy g√©r√©e + acc√®s S3 sp√©cifique suffisent.


### Donner l'acc√®s S3 √† EMR_EC2_DefaultRole pour Jupyter sur EMR

Pour permettre √† EMR de sauvegarder les notebooks Jupyter sur votre bucket S3 (ex : `oc-p11-fruits-david-scanu`), il faut accorder les droits n√©cessaires au r√¥le IAM `EMR_EC2_DefaultRole`.

#### 1. V√©rifier les policies du r√¥le

```bash
aws iam list-attached-role-policies --role-name EMR_EC2_DefaultRole
aws iam list-role-policies --role-name EMR_EC2_DefaultRole
```

#### 2. Ajouter ou mettre √† jour la policy d'acc√®s S3

```bash
aws iam put-role-policy \
	--role-name EMR_EC2_DefaultRole \
	--policy-name EMR-S3-Access \
	--policy-document '{
		"Version": "2012-10-17",
		"Statement": [
			{
				"Effect": "Allow",
				"Action": [
					"s3:GetObject",
					"s3:PutObject",
					"s3:ListBucket"
				],
				"Resource": [
					"arn:aws:s3:::oc-p11-fruits-david-scanu",
					"arn:aws:s3:::oc-p11-fruits-david-scanu/*"
				]
			}
		]
	}'
```

#### 3. V√©rifier la policy ajout√©e

```bash
aws iam get-role-policy --role-name EMR_EC2_DefaultRole --policy-name EMR-S3-Access
```

Apr√®s cette √©tape, le cluster EMR pourra lire et √©crire dans le bucket S3 pour la persistance des notebooks Jupyter.

---

## Script de cr√©ation de cluster EMR avec AWS CLI

Ce fichier documente les commandes pour ex√©cuter `create_cluster.sh`, capturer le ClusterId EMR, et les v√©rifications recommand√©es avant l'ex√©cution.   


### Avant de lancer / v√©rifications (recommand√©)

- V√©rifie ta configuration AWS CLI et tes identifiants :

```bash
aws sts get-caller-identity
```

V√©rifie qu'aucun cluster n'est encore actif :

```bash
aws emr list-clusters --active --region eu-west-1
```

Si un cluster tourne encore, termine-le imm√©diatement :

```bash
aws emr terminate-clusters --cluster-ids j-XXXXXXXXXXXXX --region eu-west-1
```

- V√©rifie la syntaxe du script sans l'ex√©cuter :

```bash
bash -n create_cluster.sh
```

- Confirme que les ARNs, groupes de s√©curit√©, paire de cl√©s et sous-r√©seau utilis√©s dans le script appartiennent √† ton compte/r√©gion. Le script fait r√©f√©rence √† des ARNs IAM, des IDs de groupes de s√©curit√©, une paire de cl√©s et un sous-r√©seau ‚Äî si l'un d'eux est incorrect, la cr√©ation du cluster √©chouera ou cr√©era des ressources dans un autre compte.

```bash
aws iam get-role --role-name EMR_DefaultRole
aws iam get-role --role-name EMR_EC2_DefaultRole
aws ec2 describe-key-pairs --key-names emr-p11-fruits-key
```

#### V√©rifier les groupes de s√©curit√© et sous-r√©seaux utilis√©s dans le script de cr√©ation de cluster

Avant de lancer `create_cluster.sh`, assure-toi que les IDs r√©f√©renc√©s existent bien dans ton compte et ta r√©gion AWS, et qu'ils appartiennent au bon VPC.

**1. V√©rifier les groupes de s√©curit√© (Security Groups)**

Dans le script, les groupes utilis√©s sont par exemple‚ÄØ:
	- `EmrManagedMasterSecurityGroup` : `sg-0ee431c02c5bc7fc4`
	- `EmrManagedSlaveSecurityGroup` : `sg-03b5c1607e57d5935`

Pour v√©rifier leur existence et leur VPC‚ÄØ:

```bash
aws ec2 describe-security-groups --group-ids sg-0ee431c02c5bc7fc4 sg-03b5c1607e57d5935 --region eu-west-1 --query 'SecurityGroups[*].[GroupId,GroupName,Description,VpcId]' --output table
```

V√©rifie que le VpcId correspond √† celui de ton sous-r√©seau et que la r√©gion est correcte.

**2. V√©rifier le sous-r√©seau (Subnet)**

Dans le script, le sous-r√©seau utilis√© est par exemple‚ÄØ:
	- `SubnetIds` : `subnet-037413c77aa8d5ebb`

Pour v√©rifier son existence, sa zone de disponibilit√© et son VPC‚ÄØ:

```bash
aws ec2 describe-subnets --subnet-ids subnet-037413c77aa8d5ebb --region eu-west-1 --query 'Subnets[*].[SubnetId,AvailabilityZone,Tags[?Key==`Name`]|[0].Value,VpcId]' --output table
```

V√©rifie que le VpcId est le m√™me que celui des groupes de s√©curit√©, et que la zone de disponibilit√© te convient.

**3. Lister tous les groupes de s√©curit√© ou sous-r√©seaux disponibles (pour choisir)**

```bash
# Groupes de s√©curit√©
aws ec2 describe-security-groups --region eu-west-1 --query 'SecurityGroups[*].[GroupId,GroupName,Description,VpcId]' --output table

# Sous-r√©seaux
aws ec2 describe-subnets --region eu-west-1 --query 'Subnets[*].[SubnetId,AvailabilityZone,Tags[?Key==`Name`]|[0].Value,VpcId]' --output table
```

**Astuce**‚ÄØ: Si tu as un doute sur le VPC utilis√©, tu peux aussi lister les VPCs‚ÄØ:

```bash
aws ec2 describe-vpcs --region eu-west-1 --query 'Vpcs[*].[VpcId,Tags[?Key==`Name`]|[0].Value,CidrBlock]' --output table
```

Cela t'assure que tous les IDs utilis√©s dans `create_cluster.sh` sont valides et coh√©rents.

- Sois conscient que cela va cr√©er des instances EC2 et peut engendrer des co√ªts. V√©rifie les types d'instances, tailles EBS et param√®tres d'Auto-termination/IdleTimeout dans le script.

- Si tu souhaites une invite de confirmation avant de cr√©er des ressources, enveloppe la commande dans une petite invite `read` ou ajoute un wrapper dry-run.

### Upload des scripts de bootstrap (si besoin)

Avant de lancer le script de cr√©ation du cluster, assure-toi que les scripts de bootstrap (ex : `install_dependencies.sh`, `set_jupyter_env.sh`) sont upload√©s dans le bucket S3 r√©f√©renc√© dans le script.

Rendre les scripts ex√©cutables (si ce n'est pas d√©j√† fait) :

```bash
chmod +x install_dependencies.sh
chmod +x set_jupyter_env.sh
```

Upload les scripts dans le bucket S3 :

```bash
aws s3 cp install_dependencies.sh s3://oc-p11-fruits-david-scanu/scripts/install_dependencies.sh
aws s3 cp set_jupyter_env.sh s3://oc-p11-fruits-david-scanu/scripts/set_jupyter_env.sh
``` 

V√©rifie qu'ils sont bien pr√©sents :

```bash
aws s3 ls s3://oc-p11-fruits-david-scanu/scripts/
```

### Proc√©dure pour configurer une cl√© SSH

G√©n√©rer la paire SSH locale (Linux / Codespace) :

```bash
mkdir -p ~/.ssh
ssh-keygen -t ed25519 -f ~/.ssh/emr-p11-fruits-key-codespace -C "emr-p11-fruits-key-codespace" -N ""
```

R√©sultat :
-- cl√© priv√©e : `~/.ssh/emr-p11-fruits-key-codespace` (garde-la secr√®te)
-- cl√© publique : `~/.ssh/emr-p11-fruits-key-codespace.pub`

```bash
# 
chmod 600 ~/.ssh/emr-p11-fruits-key-codespace
chmod 644 ~/.ssh/emr-p11-fruits-key-codespace.pub

# 
eval "$(ssh-agent -s)"
ssh-add ~/.ssh/emr-p11-fruits-key-codespace
```

Importer la cl√© publique en tant que key-pair nomm√©e `emr-p11-fruits-key-codespace` (r√©gion `eu-west-1` dans ton d√©p√¥t) :

```bash
aws ec2 import-key-pair \
  --key-name emr-p11-fruits-key-codespace \
  --public-key-material fileb://~/.ssh/emr-p11-fruits-key-codespace.pub \
  --region eu-west-1
```

V√©rifier que la key-pair existe dans la r√©gion :

```bash
aws ec2 describe-key-pairs --region eu-west-1
aws ec2 describe-key-pairs --key-names emr-p11-fruits-key --region eu-west-1
aws ec2 describe-key-pairs --key-names emr-p11-fruits-key-codespace --region eu-west-1
```

Comparer l'empreinte AWS (base64 SHA256) avec ta cl√© publique locale :

```bash
# Empreinte AWS (copie la valeur KeyFingerprint depuis describe-key-pairs)
aws ec2 describe-key-pairs --key-names emr-p11-fruits-key-codespace --region eu-west-1 --query 'KeyPairs[0].KeyFingerprint' --output text

# Empreinte locale (format compatible) :
ssh-keygen -lf ~/.ssh/emr-p11-fruits-key-codespace.pub | awk '{print $2}' | sed 's/^SHA256://'
```

#### Mettre √† jour `create_cluster.sh` pour utiliser cette cl√©

Apr√®s avoir import√© la cl√© publique avec le nom `emr-p11-fruits-key-codespace`, il faut s'assurer que le script de cr√©ation de cluster (`create_cluster.sh`) utilise ce m√™me `KeyName`. Dans `create_cluster.sh` tu trouveras la section `--ec2-attributes` contenant la paire :

	```bash
	"KeyName":"emr-p11-fruits-key",
	```

V√©rifier la valeur actuelle de KeyName dans `create_cluster.sh` :

```bash
grep -n '"KeyName"' create_cluster.sh || sed -n '1,200p' create_cluster.sh
```

Pour remplacer automatiquement le nom dans le script, ex√©cute (depuis la racine du d√©p√¥t) :

```bash
sed -i 's/"KeyName":"emr-p11-fruits-key"/"KeyName":"emr-p11-fruits-key-codespace"/' create_cluster.sh
```

V√©rifie ensuite que la modification est bien appliqu√©e :

```bash
grep -n "KeyName" create_cluster.sh || sed -n '1,200p' create_cluster.sh
```
```

### Ex√©cution rapide

Rendre le script ex√©cutable (si ce n'est pas d√©j√† fait) :

```bash
chmod +x create_cluster.sh
``` 

Ex√©cute depuis la racine du d√©p√¥t (o√π se trouve `create_cluster.sh`) :

```bash
./create_cluster.sh
```
Ou explicitement avec bash :

```bash
bash create_cluster.sh
```

La commande `aws emr create-cluster` affiche du JSON avec le nouvel id de cluster (par exemple : `{"ClusterId":"j-XXXXXXXXXXXXX"}`).

### Capturer le ClusterId (optionnel)

Si tu souhaites capturer le ClusterId dans une variable shell pour une utilisation ult√©rieure, tu peux utiliser l'une des m√©thodes suivantes.

- Utilisation de `jq` (recommand√©) :

```bash
CLUSTER_ID=$(./create_cluster.sh | jq -r '.ClusterId')
echo "Cluster cr√©√© : $CLUSTER_ID"
```

### Comment v√©rifier le statut du cluster apr√®s la cr√©ation

- Liste les clusters r√©cents :

```bash
aws emr list-clusters --cluster-states STARTING RUNNING WAITING BOOTSTRAPPING --region eu-west-1
``` 

- Exporte le ClusterId dans une variable d'environnement : 

```bash
export CLUSTER_ID=$(cat cluster_id.txt)
```

- D√©cris le cluster (remplace par l'id r√©el) :

```bash
aws emr describe-cluster --cluster-id $CLUSTER_ID --region eu-west-1
```

- Pour v√©rifier les applications install√©es sur le cluster EMR :

```bash
aws emr describe-cluster --cluster-id $(cat cluster_id.txt) --region eu-west-1 --query 'Cluster.Applications'
```

### Aides optionnelles

- Ex√©cute le script et capture la sortie de cr√©ation dans un fichier pour inspection :

```bash
./create_cluster.sh > create_cluster_output.txt
``` 

### Exemple de sortie

```txt
üöÄ Cr√©ation du cluster EMR p11-fruits-cluster...
üìç R√©gion: eu-west-1
üí∞ Configuration: 1 Master + 2 Core (m5.xlarge)


‚úÖ Cluster cr√©√© avec succ√®s !
üìã Cluster ID: j-2VLI6NTZXUAY2

üîç Pour surveiller l'√©tat:
   aws emr describe-cluster --cluster-id j-2VLI6NTZXUAY2 --region eu-west-1 --query 'Cluster.Status.State'

üåê Console AWS:
   https://eu-west-1.console.aws.amazon.com/emr/home?region=eu-west-1#/clusters/j-2VLI6NTZXUAY2

‚è∞ Attendre ~15 minutes que l'√©tat passe √† 'WAITING'

üíæ Cluster ID sauvegard√© dans: cluster_id.txt
```

### Surveiller l'√©tat du cluster EMR (`monitor_cluster.sh`)

Ce script permet de suivre en temps r√©el l'√©volution de l'√©tat du cluster EMR.

**Utilisation :**

```bash
./monitor_cluster.sh
```

- Il lit l'ID du cluster dans `cluster_id.txt` (g√©n√©r√© par `create_cluster.sh`).
- Il affiche l'√©tat du cluster (STARTING, BOOTSTRAPPING, RUNNING, WAITING, etc.) toutes les 30 secondes.
- Quand le cluster est pr√™t (`WAITING`), il affiche le DNS du master, propose la commande SSH pour acc√©der √† JupyterHub, et sauvegarde le DNS dans `master_dns.txt`.
- En cas d'arr√™t ou d'erreur, il affiche un message explicite.

### Terminer le cluster EMR (`terminate_cluster.sh`)

Ce script permet d'arr√™ter proprement le cluster EMR pour √©viter des co√ªts inutiles.

**Utilisation :**

```bash
./terminate_cluster.sh
```

- Il lit l'ID du cluster dans `cluster_id.txt`.
- Il demande une confirmation avant d'envoyer la commande d'arr√™t.
- Il affiche la commande pour surveiller la terminaison du cluster et pour v√©rifier les instances EC2 restantes.

**Bonnes pratiques :** Toujours terminer le cluster quand il n'est plus utilis√© pour √©viter des frais AWS.

---

## √âtapes suivantes apr√®s la cr√©ation du cluster EMR

Apr√®s la cr√©ation du cluster EMR et une fois qu'il est en √©tat `WAITING`, voici les √©tapes pour acc√©der √† JupyterHub, configurer la persistance S3, et commencer √† travailler avec Spark.

### Optionnel : Ajouter l‚Äô√©tape (step) pour configurer JupyterHub (si pas fait en bootstrap)

Dans `create_cluster.sh`, nous ajoutons l‚Äô√©tape pour ex√©cuter le script `set_jupyter_env.sh` depuis S3, qui configure les variables d‚Äôenvironnement n√©cessaires dans JupyterHub.

Ce script corrig√© (qui supprime proprement l‚Äôancien bloc, r√©√©crit la config, et relance le conteneur Docker JupyterHub), tu es conforme aux bonnes pratiques pour EMR + JupyterHub + S3.

Si nous uploadons ce script sur S3 et l‚Äôutilisons dans ton step EMR, la configuration sera proprement appliqu√©e √† chaque cr√©ation de cluster. Nous ne devrions plus rencontrer de probl√®me de lancement de serveur JupyterHub li√© √† la persistance S3 ou √† la variable S3_ENDPOINT_URL.

```bash
aws emr add-steps \
  --cluster-id $(cat cluster_id.txt) \
  --steps Type=CUSTOM_JAR,Name="SetJupyterEnv",ActionOnFailure=CONTINUE,Jar=command-runner.jar,Args=["bash","s3://oc-p11-fruits-david-scanu/scripts/set_jupyter_env.sh"] \
  --region eu-west-1
```

### Acc√©der √† JupyterHub

- R√©cup√©rer ton IP publique et ajouter la r√®gle (ex√©cuter apr√®s la cr√©ation du cluster) : 

```bash
# Ajouter la r√®gle
MYIP=$(curl -s https://checkip.amazonaws.com)
echo "Ton IP publique: $MYIP"
aws ec2 authorize-security-group-ingress \
  --group-id sg-0ee431c02c5bc7fc4 \
  --protocol tcp --port 22 --cidr ${MYIP}/32 \
  --region eu-west-1

# V√©rifier la r√®gle ajout√©e (liste des r√®gles ingress) : 
aws ec2 describe-security-groups --group-ids sg-0ee431c02c5bc7fc4 --region eu-west-1 --query 'SecurityGroups[0].IpPermissions' --output json

aws ec2 describe-security-groups \
  --group-ids sg-0ee431c02c5bc7fc4 \
  --region eu-west-1 \
  --query 'SecurityGroups[0].IpPermissions[?FromPort==`22` && ToPort==`22`]' \
  --output json

# Apr√®s usage, r√©voquer la r√®gle (supprimer l‚Äôentr√©e sp√©cifique) :
aws ec2 revoke-security-group-ingress \
  --group-id sg-0ee431c02c5bc7fc4 \
  --protocol tcp --port 22 --cidr ${MYIP}/32 \
  --region eu-west-1
```

- R√©cup√©rer le DNS du master EMR :

```bash
# R√©cup√©rer le DNS master (si cluster cr√©√©)
aws emr describe-cluster \
  --cluster-id $(cat cluster_id.txt) \
  --region eu-west-1 \
  --query 'Cluster.MasterPublicDnsName' \
  --output text > master_dns.txt
```

- Ouvre un tunnel SSH vers le master‚ÄØ:

Se connecter depuis le codespace : 

```bash
# Pour ouvrir le tunnel JupyterHub :
ssh -i ~/.ssh/emr-p11-fruits-key-codespace -L 9443:localhost:9443 hadoop@$(cat master_dns.txt)

# Se connecter (depuis ce Codespace si la cl√© priv√©e y est) :
ssh -i ~/.ssh/emr-p11-fruits-key-codespace hadoop@$(cat master_dns.txt)
```

Se connecter depuis mon environnement local :

```bash
ssh -i ~/.ssh/emr-p11-fruits-key.pem -L 9443:localhost:9443 hadoop@$(cat master_dns.txt)
```

### Modifier la configuration de JupyterHub pour S3 et Spark (si besoin)

V√©rifier que la variable d‚Äôenvironnement est bien prise en compte dans le conteneur JupyterHub‚ÄØ:

```bash
sudo cat /etc/jupyter/conf/jupyterhub_config.py
```

Voici la commande unique, propre et corrig√©e √† utiliser juste apr√®s ta connexion SSH pour √©craser proprement la configuration d‚Äôenvironnement JupyterHub‚ÄØ:

```bash
sudo cp /etc/jupyter/conf/jupyterhub_config.py /etc/jupyter/conf/jupyterhub_config.py.bak.$(date -u +"%Y%m%dT%H%M%SZ")
ls -l /etc/jupyter/conf/jupyterhub_config.py*
```

```bash
# Upload du fichier dans la machine EMR (depuis votre machine locale / Codespace)
scp -i ~/.ssh/emr-p11-fruits-key-codespace /workspaces/oc-ai-engineer-p11-realisez-traitement-environnement-big-data-cloud/jupyterhub_config_working.py hadoop@$(cat master_dns.txt):/tmp/

# ensuite, sur l'EMR (ou via ssh -i ... hadoop@$(cat master_dns.txt) '...'):
sudo mv /tmp/jupyterhub_config_working.py /etc/jupyter/conf/jupyterhub_config.py
sudo chown root:root /etc/jupyter/conf/jupyterhub_config.py
sudo chmod 644 /etc/jupyter/conf/jupyterhub_config.py

# V√©rifier que le fichier de configuration a bien √©t√© upload√©
sudo cat /etc/jupyter/conf/jupyterhub_config.py
```

Red√©marrer le conteneur Docker : 

```bash
# Red√©marrage du conteneur
sudo docker restart jupyterhub
# V√©rification de l'√©tat du conteneur
sudo docker ps --filter "name=jupyterhub" --format 'table {{.ID}}\t{{.Names}}\t{{.Status}}'
# V√©rifier que le port 9443 est bien √©cout√© :
sudo ss -ltnp | grep 9443 || sudo netstat -ltnp | grep 9443
# V√©rifier les logs en cas de probl√®me de spawn
sudo docker logs jupyterhub
# ou 
sudo docker logs -f jupyterhub
```

### Uploader le notebook de travail dans S3 pour la persistance 

- Avant de commencer √† travailler, upload le notebook `notebooks/p11-david-scanu-EMR-production.ipynb` dans ton bucket S3 pour t'assurer que la persistance fonctionne correctement‚ÄØ:

```bash
aws s3 cp notebooks/p11-david-scanu-EMR-production.ipynb s3://oc-p11-fruits-david-scanu/jupyter/jovyan/p11-david-scanu-EMR-production.ipynb --acl bucket-owner-full-control
```
- Puis v√©rifie qu‚Äôil appara√Æt dans le bucket S3‚ÄØ:

```bash
aws s3 ls s3://oc-p11-fruits-david-scanu/jupyter/jovyan/
``` 

En alternative, cr√©e un notebook directement depuis JupyterHub et v√©rifie qu‚Äôil est bien sauvegard√© dans S3.

- Si besoin, importe `notebooks/p11-david-scanu-EMR-production.ipynb` via le bouton "Upload" de JupyterHub (https://localhost:9443).
- Sinon, cr√©e un nouveau notebook en choisissant le kernel PySpark et travaille directement dedans ; v√©rifie ensuite que le fichier appara√Æt dans ton bucket S3.

### Connexion √† JupyterHub

- Dans ton navigateur, ouvre‚ÄØ: https://localhost:9443
- **Identifiants JupyterHub par d√©faut**‚ÄØ:
	- Username : `jovyan`
	- Password : `jupyter`
- Ces identifiants sont ceux utilis√©s par d√©faut dans de nombreux d√©ploiements JupyterHub Docker (notamment sur EMR), pour simplifier l'acc√®s initial. Pour un usage s√©curis√©, il est recommand√© de les modifier ou de configurer une authentification plus robuste.

### 4. Charger et explorer les donn√©es

- S√©lectionner le kernel **PySpark** fourni par EMR (et non un kernel Python classique).
- Ouvre ce notebook pour ex√©cuter le pipeline et lire les donn√©es depuis S3.

#### **Ajustements et v√©rifications pratiques**

- Cr√©e toujours un notebook avec le kernel **PySpark** EMR pour b√©n√©ficier de Spark et Java pr√©configur√©s.
- Utilise le pr√©fixe `s3a://` pour tous les acc√®s S3 avec Spark.
- V√©rifie la disponibilit√© de la SparkSession et la version de Spark :
  ```python
  print("SparkSession disponible :", 'spark' in globals())
  print("Version Spark :", spark.version)
  ```

#### Pour lire les donn√©es depuis S3‚ÄØ:

##### Avec pandas (pour des petits fichiers CSV ou images)‚ÄØ:

```python
import pandas as pd
df = pd.read_csv('s3://oc-p11-fruits-david-scanu/data/raw/Training/Apple Braeburn/0_100.jpg')
```

##### Avec PySpark :

- Pour tester la lecture d'une image en binaire avec Spark‚ÄØ:
```python
# Pour tester la lecture d'une image en binaire avec Spark
df = spark.read.format("binaryFile").load("s3a://oc-p11-fruits-david-scanu/data/raw/Training/Apple Braeburn/0_100.jpg")
df.show()
```
- Pour charger un grand nombre de fichiers ou traiter en distribu√©, ou lire des images :
```python
# Utilise bien le pr√©fixe s3a://
df = spark.read.format('csv').load('s3a://oc-p11-fruits-david-scanu/data/raw/Training/*')
# Pour lire des images ou lister les fichiers :
s3_path = "s3a://oc-p11-fruits-david-scanu/data/raw/Training/*/*"
df = spark.read.format("binaryFile").load(s3_path)
df.select("path").show(5, truncate=False)
```
- Pour lister des fichiers sur S3 (et v√©rifier l'acc√®s S3 via Spark), utilise :
```python
s3_path = "s3a://oc-p11-fruits-david-scanu/data/raw/Training/*/*"
df = spark.read.format("binaryFile").load(s3_path)
df.select("path").show(5, truncate=False)
```

Si tu vois des chemins S3 s'afficher, l'acc√®s S3 via Spark est valid√©. Les notebooks et r√©sultats sauvegard√©s seront automatiquement stock√©s dans S3 sous `s3://oc-p11-fruits-david-scanu/jupyter/jovyan/`.

### 5. **Lancer tes traitements Spark ou analyses**
	- Ex√©cute tes scripts ou notebooks de traitement, d‚Äôanalyse ou de machine learning.

### 6. **Sauvegarder les r√©sultats**
	- √âcris les r√©sultats (CSV, mod√®les, etc.) dans un dossier d√©di√© sur S3.

### 7. **Arr√™ter le cluster quand tu as termin√©**
	- Pour √©viter des co√ªts inutiles‚ÄØ:
	  ```bash
	  ./terminate_cluster.sh
	  ```

**Bonnes pratiques**‚ÄØ: Sauvegarde r√©guli√®rement tes notebooks, surveille l‚Äôutilisation des ressources, et arr√™te toujours le cluster apr√®s usage.

---

## D√©bogage de la persistance S3 et de JupyterHub sur EMR

Si le serveur JupyterHub d√©marre mais que la cr√©ation d'un notebook √©choue (erreur lors du spawn du serveur utilisateur), il s'agit souvent d'un probl√®me de configuration de l'acc√®s S3 ou du point d'acc√®s (endpoint) S3.

### Sympt√¥mes typiques
- Impossible de cr√©er ou sauvegarder un notebook (erreur 500 ou message d'√©chec dans JupyterHub)
- Logs JupyterHub/Docker mentionnant des erreurs d'acc√®s S3 ou d'endpoint

### √âtapes de diagnostic et de r√©solution

1. **V√©rifier les logs JupyterHub**

  - Se connecter en SSH sur le master du cluster EMR (environnement local):
    ```bash
    ssh -i ~/.ssh/emr-p11-fruits-key.pem hadoop@$(cat master_dns.txt)
    ```
  - Se connecter en SSH sur le master du cluster EMR (environnement codespace):
    ```bash
    ssh -i ~/.ssh/emr-p11-fruits-key-codespace hadoop@$(cat master_dns.txt)
    ```
  - Consulter les logs :
    ```bash
    sudo cat /var/log/jupyter/jupyter.log | tail -n 100
    ```

2. **V√©rifier la configuration du point d'acc√®s S3**

	 - Le fichier de configuration se trouve g√©n√©ralement ici :
    ```bash
    sudo cat /etc/jupyter/conf/jupyterhub_config.py
    ```
	 - V√©rifier la ligne suivante :
    ```python
    c.Spawner.environment['S3_ENDPOINT_URL'] = 'https://s3.eu-west-1.amazonaws.com'
    ```
	 - L'endpoint doit correspondre √† la r√©gion de votre bucket S3 (ex : `eu-west-1`).

3. **Modifier la configuration si besoin**
	 - Pour corriger l'endpoint, utiliser la commande suivante :
    ```bash
    sudo sed -i "s|s3_endpoint_url = os.environ.get('S3_ENDPOINT_URL', .*|s3_endpoint_url = os.environ.get('S3_ENDPOINT_URL', 's3.eu-west-1.amazonaws.com')|" /etc/jupyter/conf/jupyterhub_config.py
    ```

4. **Activer le mode debug pour JupyterHub**
	 - Ajouter ou modifier la ligne suivante dans le m√™me fichier :
		 ```python
		 sudo sed -i "s|^c.JupyterHub.log_level *=.*|c.JupyterHub.log_level = 'DEBUG'|" /etc/jupyter/conf/jupyterhub_config.py
		 ```

5. **Red√©marrer le conteneur JupyterHub**
	 - Trouver le nom du conteneur (ex : `jupyterhub` ou similaire) :
		 ```bash
		 sudo docker ps
		 ```
	 - Red√©marrer le conteneur :
		 ```bash
		 sudo docker restart jupyterhub
		 ```
	 - V√©rifier qu'il est bien relanc√© :
		 ```bash
		 sudo docker ps
		 ```

6. **R√©essayer la connexion √† JupyterHub**
	 - Ouvre √† nouveau le tunnel SSH et connecte-toi √† https://localhost:9443
	 - Tente de cr√©er un notebook et v√©rifie la persistance S3

### Bonnes pratiques
- Toujours v√©rifier que l'endpoint S3 correspond √† la r√©gion du bucket
- Activer le mode debug pour obtenir plus d'informations dans les logs
- Apr√®s modification de la configuration, toujours red√©marrer le conteneur JupyterHub