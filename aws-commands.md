
# Commandes AWS utiles pour EMR et IAM

Toutes les commandes AWS CLI document√©es ici sont √† ex√©cuter dans un terminal o√π l‚ÄôAWS CLI est install√©e et configur√©e avec des identifiants ayant les droits n√©cessaires (IAM user avec permissions EMR, S3, IAM selon les besoins).

---

## Upload du dataset vers S3

Pour transf√©rer ton dataset local vers le bucket S3 (exemple : `oc-p11-fruits-david-scanu`), tu peux utiliser la commande suivante‚ÄØ:

```bash
aws s3 cp --recursive ./data/raw/fruits-360_dataset/fruits-360/Training/ \
	s3://oc-p11-fruits-david-scanu/data/raw/Training/
```

Cette commande copie tout le dossier `Training` (et son contenu) dans le bucket S3, en conservant l‚Äôarborescence.

Pour v√©rifier l‚Äôupload‚ÄØ:

```bash
aws s3 ls s3://oc-p11-fruits-david-scanu/data/raw/Training/Apple\ Braeburn/
```

**Remarque**‚ÄØ: Les espaces dans les noms de dossiers/fichiers doivent √™tre √©chapp√©s avec un antislash (`\`) ou entour√©s de guillemets.

---

## Autorisations n√©cessaires pour EMR_EC2_DefaultRole (R√©sum√©)

Le r√¥le `EMR_EC2_DefaultRole` doit avoir les autorisations suivantes pour un fonctionnement optimal du cluster EMR‚ÄØ:

- **Acc√®s S3**‚ÄØ: lecture/√©criture sur les buckets utilis√©s (logs, scripts, donn√©es, persistance Jupyter).
- **Policy g√©r√©e AmazonElasticMapReduceforEC2Role**‚ÄØ: donne acc√®s √† EC2, CloudWatch, S3 (certains usages), DynamoDB, KMS, etc. (d√©j√† attach√©e par d√©faut).
- **CloudWatch Logs**‚ÄØ: pour √©crire les logs Spark/Hadoop/Jupyter (inclus dans la policy g√©r√©e).
- **KMS**‚ÄØ: si tu utilises des buckets S3 chiffr√©s avec KMS.
- **DynamoDB**‚ÄØ: si tu utilises EMRFS consistent view.
- **Glue**‚ÄØ: si tu utilises le Data Catalog Glue comme metastore Hive/Spark.
- **Autres (optionnel)**‚ÄØ: `CloudWatchAgentServerPolicy` (monitoring avanc√©), `AmazonSSMManagedInstanceCore` (acc√®s SSM/Session Manager).

Pour la majorit√© des cas EMR Spark/Hadoop/Jupyter, la policy g√©r√©e + acc√®s S3 sp√©cifique suffisent.


### Donner l'acc√®s S3 √† EMR_EC2_DefaultRole pour Jupyter sur EMR

Pour permettre √† EMR de sauvegarder les notebooks Jupyter sur votre bucket S3 (ex : `oc-p11-fruits-david-scanu`), il faut accorder les droits n√©cessaires au r√¥le IAM `EMR_EC2_DefaultRole`.

#### 1. V√©rifier les policies du r√¥le

```bash
aws iam list-attached-role-policies --role-name EMR_EC2_DefaultRole
aws iam list-role-policies --role-name EMR_EC2_DefaultRole
```

#### 2. Ajouter ou mettre √† jour la policy d'acc√®s S3

```bash
aws iam put-role-policy \
	--role-name EMR_EC2_DefaultRole \
	--policy-name EMR-S3-Access \
	--policy-document '{
		"Version": "2012-10-17",
		"Statement": [
			{
				"Effect": "Allow",
				"Action": [
					"s3:GetObject",
					"s3:PutObject",
					"s3:ListBucket"
				],
				"Resource": [
					"arn:aws:s3:::oc-p11-fruits-david-scanu",
					"arn:aws:s3:::oc-p11-fruits-david-scanu/*"
				]
			}
		]
	}'
```

#### 3. V√©rifier la policy ajout√©e

```bash
aws iam get-role-policy --role-name EMR_EC2_DefaultRole --policy-name EMR-S3-Access
```

Apr√®s cette √©tape, le cluster EMR pourra lire et √©crire dans le bucket S3 pour la persistance des notebooks Jupyter.

---

## Script de cr√©ation de cluster EMR avec AWS CLI

Ce fichier documente les commandes pour ex√©cuter `create_cluster.sh`, capturer le ClusterId EMR, et les v√©rifications recommand√©es avant l'ex√©cution.   


### Avant de lancer / v√©rifications (recommand√©)

V√©rifie qu'aucun cluster n'est encore actif :

```bash
aws emr list-clusters --active --region eu-west-1
```

Si un cluster tourne encore, termine-le imm√©diatement :

```bash
aws emr terminate-clusters --cluster-ids j-XXXXXXXXXXXXX --region eu-west-1
```

- V√©rifie la syntaxe du script sans l'ex√©cuter :

```bash
bash -n create_cluster.sh
```

- V√©rifie ta configuration AWS CLI et tes identifiants :

```bash
aws sts get-caller-identity
```

- Confirme que les ARNs, groupes de s√©curit√©, paire de cl√©s et sous-r√©seau utilis√©s dans le script appartiennent √† ton compte/r√©gion. Le script fait r√©f√©rence √† des ARNs IAM, des IDs de groupes de s√©curit√©, une paire de cl√©s et un sous-r√©seau ‚Äî si l'un d'eux est incorrect, la cr√©ation du cluster √©chouera ou cr√©era des ressources dans un autre compte.

- Sois conscient que cela va cr√©er des instances EC2 et peut engendrer des co√ªts. V√©rifie les types d'instances, tailles EBS et param√®tres d'Auto-termination/IdleTimeout dans le script.

- Si tu souhaites une invite de confirmation avant de cr√©er des ressources, enveloppe la commande dans une petite invite `read` ou ajoute un wrapper dry-run.

### Upload des scripts de bootstrap (si besoin)

Avant de lancer le script de cr√©ation du cluster, assure-toi que les scripts de bootstrap (ex : `install_dependencies.sh`, `set_jupyter_env.sh`) sont upload√©s dans le bucket S3 r√©f√©renc√© dans le script.

Rendre les scripts ex√©cutables (si ce n'est pas d√©j√† fait) :

```bash
chmod +x install_dependencies.sh
chmod +x set_jupyter_env.sh
```

Upload les scripts dans le bucket S3 :

```bash
aws s3 cp install_dependencies.sh s3://oc-p11-fruits-david-scanu/scripts/install_dependencies.sh
aws s3 cp set_jupyter_env.sh s3://oc-p11-fruits-david-scanu/scripts/set_jupyter_env.sh
``` 

V√©rifie qu'ils sont bien pr√©sents :

```bash
aws s3 ls s3://oc-p11-fruits-david-scanu/scripts/
```

### Ex√©cution rapide

Rendre le script ex√©cutable (si ce n'est pas d√©j√† fait) :

```bash
chmod +x create_cluster.sh
``` 

Ex√©cute depuis la racine du d√©p√¥t (o√π se trouve `create_cluster.sh`) :

```bash
./create_cluster.sh
```
Ou explicitement avec bash :

```bash
bash create_cluster.sh
```

La commande `aws emr create-cluster` affiche du JSON avec le nouvel id de cluster (par exemple : `{"ClusterId":"j-XXXXXXXXXXXXX"}`).

### Capturer le ClusterId (optionnel)

Si tu souhaites capturer le ClusterId dans une variable shell pour une utilisation ult√©rieure, tu peux utiliser l'une des m√©thodes suivantes.

- Utilisation de `jq` (recommand√©) :

```bash
CLUSTER_ID=$(./create_cluster.sh | jq -r '.ClusterId')
echo "Cluster cr√©√© : $CLUSTER_ID"
```

### Comment v√©rifier le statut du cluster apr√®s la cr√©ation

- Liste les clusters r√©cents :

```bash
aws emr list-clusters --cluster-states STARTING RUNNING WAITING BOOTSTRAPPING --region eu-west-1
``` 

- Exporte le ClusterId dans une variable d'environnement : 

```bash
export CLUSTER_ID=$(cat cluster_id.txt)
```

- D√©cris le cluster (remplace par l'id r√©el) :

```bash
aws emr describe-cluster --cluster-id $CLUSTER_ID --region eu-west-1
```

- Pour v√©rifier les applications install√©es sur le cluster EMR :

```bash
aws emr describe-cluster --cluster-id $(cat cluster_id.txt) --region eu-west-1 --query 'Cluster.Applications'
```

### Aides optionnelles

- Ex√©cute le script et capture la sortie de cr√©ation dans un fichier pour inspection :

```bash
./create_cluster.sh > create_cluster_output.txt
``` 

### Exemple de sortie

```txt
üöÄ Cr√©ation du cluster EMR p11-fruits-cluster...
üìç R√©gion: eu-west-1
üí∞ Configuration: 1 Master + 2 Core (m5.xlarge)


‚úÖ Cluster cr√©√© avec succ√®s !
üìã Cluster ID: j-2VLI6NTZXUAY2

üîç Pour surveiller l'√©tat:
   aws emr describe-cluster --cluster-id j-2VLI6NTZXUAY2 --region eu-west-1 --query 'Cluster.Status.State'

üåê Console AWS:
   https://eu-west-1.console.aws.amazon.com/emr/home?region=eu-west-1#/clusters/j-2VLI6NTZXUAY2

‚è∞ Attendre ~15 minutes que l'√©tat passe √† 'WAITING'

üíæ Cluster ID sauvegard√© dans: cluster_id.txt
```

### Surveiller l'√©tat du cluster EMR (`monitor_cluster.sh`)

Ce script permet de suivre en temps r√©el l'√©volution de l'√©tat du cluster EMR.

**Utilisation :**

```bash
./monitor_cluster.sh
```

- Il lit l'ID du cluster dans `cluster_id.txt` (g√©n√©r√© par `create_cluster.sh`).
- Il affiche l'√©tat du cluster (STARTING, BOOTSTRAPPING, RUNNING, WAITING, etc.) toutes les 30 secondes.
- Quand le cluster est pr√™t (`WAITING`), il affiche le DNS du master, propose la commande SSH pour acc√©der √† JupyterHub, et sauvegarde le DNS dans `master_dns.txt`.
- En cas d'arr√™t ou d'erreur, il affiche un message explicite.

### Terminer le cluster EMR (`terminate_cluster.sh`)

Ce script permet d'arr√™ter proprement le cluster EMR pour √©viter des co√ªts inutiles.

**Utilisation :**

```bash
./terminate_cluster.sh
```

- Il lit l'ID du cluster dans `cluster_id.txt`.
- Il demande une confirmation avant d'envoyer la commande d'arr√™t.
- Il affiche la commande pour surveiller la terminaison du cluster et pour v√©rifier les instances EC2 restantes.

**Bonnes pratiques :** Toujours terminer le cluster quand il n'est plus utilis√© pour √©viter des frais AWS.

---

## √âtapes suivantes apr√®s la cr√©ation du cluster EMR


### 1. **R√©cup√©rer le DNS du master EMR**

```bash
aws emr describe-cluster \
  --cluster-id $(cat cluster_id.txt) \
  --region eu-west-1 \
  --query 'Cluster.MasterPublicDnsName' \
  --output text > master_dns.txt
```

### 2. **Acc√©der √† JupyterHub**
	- Ouvre un tunnel SSH vers le master‚ÄØ:
	  ```bash
	  ssh -i ~/.ssh/emr-p11-fruits-key.pem -L 9443:localhost:9443 hadoop@$(cat master_dns.txt)
	  ```
	- Dans ton navigateur, ouvre‚ÄØ: https://localhost:9443
	- **Identifiants JupyterHub par d√©faut**‚ÄØ:
	  - Username : `jovyan`
	  - Password : `jupyter`
	- Ces identifiants sont ceux utilis√©s par d√©faut dans de nombreux d√©ploiements JupyterHub Docker (notamment sur EMR), pour simplifier l'acc√®s initial. Pour un usage s√©curis√©, il est recommand√© de les modifier ou de configurer une authentification plus robuste.

### 3. Uploader le notebook de travail dans S3 pour la persistance 

- Avant de commencer √† travailler, upload le notebook `notebooks/p11-david-scanu-EMR-production.ipynb` dans ton bucket S3 pour t'assurer que la persistance fonctionne correctement‚ÄØ:

```bash
aws s3 cp notebooks/p11-david-scanu-EMR-production.ipynb s3://oc-p11-fruits-david-scanu/jupyter/jovyan/p11-david-scanu-EMR-production.ipynb --acl bucket-owner-full-control
```
- Puis v√©rifie qu‚Äôil appara√Æt dans le bucket S3‚ÄØ:

```bash
aws s3 ls s3://oc-p11-fruits-david-scanu/jupyter/jovyan/
``` 

En alternative, cr√©e un notebook directement depuis JupyterHub et v√©rifie qu‚Äôil est bien sauvegard√© dans S3.

- Si besoin, importe `notebooks/p11-david-scanu-EMR-production.ipynb` via le bouton "Upload" de JupyterHub (https://localhost:9443).
- Sinon, cr√©e un nouveau notebook en choisissant le kernel PySpark et travaille directement dedans ; v√©rifie ensuite que le fichier appara√Æt dans ton bucket S3.

### 4. **Charger et explorer les donn√©es**

- S√©lectionner le kernel **PySpark** fourni par EMR (et non un kernel Python classique).
- Ouvre ce notebook pour ex√©cuter le pipeline et lire les donn√©es depuis S3.

#### **Ajustements et v√©rifications pratiques**

- Cr√©e toujours un notebook avec le kernel **PySpark** EMR pour b√©n√©ficier de Spark et Java pr√©configur√©s.
- Utilise le pr√©fixe `s3a://` pour tous les acc√®s S3 avec Spark.
- V√©rifie la disponibilit√© de la SparkSession et la version de Spark :
  ```python
  print("SparkSession disponible :", 'spark' in globals())
  print("Version Spark :", spark.version)
  ```

#### Pour lire les donn√©es depuis S3‚ÄØ:

##### Avec pandas (pour des petits fichiers CSV ou images)‚ÄØ:

```python
import pandas as pd
df = pd.read_csv('s3://oc-p11-fruits-david-scanu/data/raw/Training/Apple Braeburn/0_100.jpg')
```

##### Avec PySpark :

- Pour tester la lecture d'une image en binaire avec Spark‚ÄØ:
```python
# Pour tester la lecture d'une image en binaire avec Spark
df = spark.read.format("binaryFile").load("s3a://oc-p11-fruits-david-scanu/data/raw/Training/Apple Braeburn/0_100.jpg")
df.show()
```
- Pour charger un grand nombre de fichiers ou traiter en distribu√©, ou lire des images :
```python
# Utilise bien le pr√©fixe s3a://
df = spark.read.format('csv').load('s3a://oc-p11-fruits-david-scanu/data/raw/Training/*')
# Pour lire des images ou lister les fichiers :
s3_path = "s3a://oc-p11-fruits-david-scanu/data/raw/Training/*/*"
df = spark.read.format("binaryFile").load(s3_path)
df.select("path").show(5, truncate=False)
```
- Pour lister des fichiers sur S3 (et v√©rifier l'acc√®s S3 via Spark), utilise :
```python
s3_path = "s3a://oc-p11-fruits-david-scanu/data/raw/Training/*/*"
df = spark.read.format("binaryFile").load(s3_path)
df.select("path").show(5, truncate=False)
```

Si tu vois des chemins S3 s'afficher, l'acc√®s S3 via Spark est valid√©. Les notebooks et r√©sultats sauvegard√©s seront automatiquement stock√©s dans S3 sous `s3://oc-p11-fruits-david-scanu/jupyter/jovyan/`.

### 5. **Lancer tes traitements Spark ou analyses**
	- Ex√©cute tes scripts ou notebooks de traitement, d‚Äôanalyse ou de machine learning.

### 6. **Sauvegarder les r√©sultats**
	- √âcris les r√©sultats (CSV, mod√®les, etc.) dans un dossier d√©di√© sur S3.

### 7. **Arr√™ter le cluster quand tu as termin√©**
	- Pour √©viter des co√ªts inutiles‚ÄØ:
	  ```bash
	  ./terminate_cluster.sh
	  ```

**Bonnes pratiques**‚ÄØ: Sauvegarde r√©guli√®rement tes notebooks, surveille l‚Äôutilisation des ressources, et arr√™te toujours le cluster apr√®s usage.


---

## D√©bogage de la persistance S3 et de JupyterHub sur EMR

Si le serveur JupyterHub d√©marre mais que la cr√©ation d'un notebook √©choue (erreur lors du spawn du serveur utilisateur), il s'agit souvent d'un probl√®me de configuration de l'acc√®s S3 ou du point d'acc√®s (endpoint) S3.

### Sympt√¥mes typiques
- Impossible de cr√©er ou sauvegarder un notebook (erreur 500 ou message d'√©chec dans JupyterHub)
- Logs JupyterHub/Docker mentionnant des erreurs d'acc√®s S3 ou d'endpoint

### √âtapes de diagnostic et de r√©solution

1. **V√©rifier les logs JupyterHub**

  - Se connecter en SSH sur le master du cluster EMR :
    ```bash
    ssh -i ~/.ssh/emr-p11-fruits-key.pem hadoop@$(cat master_dns.txt)
    ```
  - Consulter les logs :
    ```bash
    sudo cat /var/log/jupyter/jupyter.log | tail -n 100
    ```


2. **V√©rifier la configuration du point d'acc√®s S3**
	 - Le fichier de configuration se trouve g√©n√©ralement ici :
    ```bash
    sudo cat /etc/jupyter/conf/jupyterhub_config.py
    ```
	 - V√©rifier la ligne suivante :
    ```python
    c.Spawner.environment['S3_ENDPOINT_URL'] = 'https://s3.eu-west-1.amazonaws.com'
    ```

	 - L'endpoint doit correspondre √† la r√©gion de votre bucket S3 (ex : `eu-west-1`).

3. **Modifier la configuration si besoin**
	 - Pour corriger l'endpoint, utiliser la commande suivante :
    ```bash
    sudo sed -i "s|s3_endpoint_url = os.environ.get('S3_ENDPOINT_URL', .*|s3_endpoint_url = os.environ.get('S3_ENDPOINT_URL', 'https://s3.eu-west-1.amazonaws.com')|" /etc/jupyter/conf/jupyterhub_config.py
    ```

4. **Activer le mode debug pour JupyterHub**
	 - Ajouter ou modifier la ligne suivante dans le m√™me fichier :
		 ```python
		 sudo sed -i "s|^c.JupyterHub.log_level *=.*|c.JupyterHub.log_level = 'DEBUG'|" /etc/jupyter/conf/jupyterhub_config.py
		 ```

5. **Red√©marrer le conteneur JupyterHub**
	 - Trouver le nom du conteneur (ex : `jupyterhub` ou similaire) :
		 ```bash
		 sudo docker ps
		 ```
	 - Red√©marrer le conteneur :
		 ```bash
		 sudo docker restart jupyterhub
		 ```
	 - V√©rifier qu'il est bien relanc√© :
		 ```bash
		 sudo docker ps
		 ```

6. **R√©essayer la connexion √† JupyterHub**
	 - Ouvre √† nouveau le tunnel SSH et connecte-toi √† https://localhost:9443
	 - Tente de cr√©er un notebook et v√©rifie la persistance S3

### Bonnes pratiques
- Toujours v√©rifier que l'endpoint S3 correspond √† la r√©gion du bucket
- Activer le mode debug pour obtenir plus d'informations dans les logs
- Apr√®s modification de la configuration, toujours red√©marrer le conteneur JupyterHub