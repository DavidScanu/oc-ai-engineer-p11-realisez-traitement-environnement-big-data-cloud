# Workflow d√©taill√© - √âtape 1

## üéØ Vue d'ensemble du workflow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PR√âPARATION (Local)                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                   ‚îÇ                   ‚îÇ
    ‚ñº                   ‚ñº                   ‚ñº
[√âditer]          [V√©rifier]          [Uploader]
config.sh         verify_setup.sh     upload_scripts.sh
    ‚îÇ                   ‚îÇ                   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   CR√âATION CLUSTER (AWS)                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
                create_cluster.sh
                        ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                               ‚îÇ
        ‚ñº                               ‚ñº
  [AWS EMR API]                  [Surveiller]
  Cr√©er cluster               monitor_cluster.sh
        ‚îÇ                               ‚îÇ
        ‚îú‚îÄ‚ñ∫ STARTING                    ‚îÇ
        ‚îú‚îÄ‚ñ∫ BOOTSTRAPPING ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ
        ‚îÇ   (install_dependencies.sh)   ‚îÇ
        ‚îú‚îÄ‚ñ∫ RUNNING                     ‚îÇ
        ‚îî‚îÄ‚ñ∫ WAITING ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  EX√âCUTION JOB (AWS EMR)                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
                  submit_job.sh
                        ‚îÇ
                        ‚ñº
              [Spark Submit Step]
                        ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                                ‚îÇ
        ‚ñº                                ‚ñº
  read_fruits_data.py            [Spark Cluster]
        ‚îÇ                                ‚îÇ
        ‚îú‚îÄ‚ñ∫ Read S3 images               ‚îÇ
        ‚îú‚îÄ‚ñ∫ Extract metadata ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ
        ‚îú‚îÄ‚ñ∫ Calculate stats              ‚îÇ
        ‚îî‚îÄ‚ñ∫ Write CSV to S3              ‚îÇ
                        ‚îÇ
                        ‚ñº
                 Step COMPLETED
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   R√âCUP√âRATION (Local)                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
              [Download from S3]
              aws s3 cp ...
                        ‚îÇ
                        ‚ñº
                Analyse des r√©sultats
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    NETTOYAGE (AWS)                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
              terminate_cluster.sh
                        ‚îÇ
                        ‚ñº
              Cluster TERMINATED
```

## üìã √âtapes d√©taill√©es

### 1. PR√âPARATION

#### 1.1 Configuration initiale

```bash
cd traitement/etape_1
nano config/config.sh
```

**Variables √† modifier** :
- `S3_BUCKET` : Nom de votre bucket S3
- `EC2_KEY_NAME` : Nom de votre cl√© SSH EC2
- `MASTER_SECURITY_GROUP` : ID du security group master
- `SLAVE_SECURITY_GROUP` : ID du security group slave
- `EC2_SUBNET` : ID du subnet VPC
- `IAM_SERVICE_ROLE` : ARN du r√¥le EMR_DefaultRole
- `IAM_INSTANCE_PROFILE` : Nom du instance profile
- `IAM_AUTOSCALING_ROLE` : ARN du r√¥le d'auto-scaling

#### 1.2 V√©rification de la configuration

```bash
./scripts/verify_setup.sh
```

**V√©rifications effectu√©es** :
1. ‚úÖ R√©gion AWS (Europe pour GDPR)
2. ‚úÖ Credentials AWS valides
3. ‚úÖ Bucket S3 existe et accessible
4. ‚úÖ Donn√©es d'entr√©e pr√©sentes (fichiers .jpg)
5. ‚úÖ Cl√© SSH existe dans EC2
6. ‚úÖ R√¥les IAM configur√©s

#### 1.3 Upload des scripts

```bash
./scripts/upload_scripts.sh
```

**Fichiers upload√©s** :
1. `install_dependencies.sh` ‚Üí `s3://bucket/scripts/`
2. `read_fruits_data.py` ‚Üí `s3://bucket/scripts/`

---

### 2. CR√âATION DU CLUSTER

#### 2.1 Lancement de la cr√©ation

```bash
./scripts/create_cluster.sh
```

**Param√®tres du cluster** :
- Nom : `p11-fruits-etape1`
- Version EMR : `7.11.0`
- Applications : Spark, Hadoop
- Master : 1x m5.xlarge
- Core : 2x m5.xlarge
- Bootstrap : `install_dependencies.sh`
- Auto-terminaison : 4h

**Sortie** :
```
üìã Cluster ID: j-XXXXXXXXXXXXX
üíæ Cluster ID sauvegard√© dans: cluster_id.txt
```

#### 2.2 Surveillance du d√©marrage

```bash
./scripts/monitor_cluster.sh
```

**√âtats du cluster** :
```
[08:00:00] üü° STARTING - D√©marrage des instances EC2...
[08:05:00] üü° BOOTSTRAPPING - Installation des d√©pendances Python...
[08:10:00] üü¢ RUNNING - Configuration en cours...
[08:15:00] ‚úÖ WAITING - Cluster pr√™t √† l'emploi !
```

**Dur√©e totale** : ~10-15 minutes

---

### 3. EX√âCUTION DU JOB PYSPARK

#### 3.1 Soumission du step

```bash
./scripts/submit_job.sh
```

**Commande Spark Submit** :
```bash
spark-submit \
  --deploy-mode cluster \
  --master yarn \
  --conf spark.executorEnv.PYTHONHASHSEED=0 \
  s3://bucket/scripts/read_fruits_data.py \
  s3://bucket/data/fruits-360/ \
  s3://bucket/output/etape_1/
```

**Sortie** :
```
üìã Step ID: s-XXXXXXXXXXXXX
üíæ Step ID sauvegard√© dans: step_id.txt
```

#### 3.2 Ex√©cution distribu√©e

**Sur le Master (Driver)** :
1. T√©l√©charge `read_fruits_data.py` depuis S3
2. Initialise SparkSession
3. Coordonne les Executors

**Sur les Core Nodes (Executors)** :
1. Lisent les partitions d'images depuis S3
2. Ex√©cutent les transformations (map, filter, groupBy)
3. √âcrivent les r√©sultats partitionn√©s

**Pipeline de traitement** :
```python
# 1. Lecture (binaryFile)
df_files = spark.read.format("binaryFile").load("s3://...")

# 2. Extraction m√©tadonn√©es (regex)
df_metadata = df_files.select(
    regexp_extract(col("path"), r"/([^/]+)/[^/]+\.jpg$", 1).alias("label"),
    ...
)

# 3. Statistiques (groupBy)
df_stats = df_metadata.groupBy("split", "label").count()

# 4. Sauvegarde (CSV)
df_metadata.coalesce(1).write.csv("s3://...")
```

#### 3.3 Surveillance de l'ex√©cution

```bash
# Option 1 : Surveillance continue
watch -n 10 'aws emr describe-step --cluster-id $(cat cluster_id.txt) --step-id $(cat step_id.txt) --region eu-west-1 --query "Step.Status.State" --output text'

# Option 2 : V√©rification ponctuelle
aws emr describe-step --cluster-id $(cat cluster_id.txt) --step-id $(cat step_id.txt) --region eu-west-1
```

**√âtats du step** :
```
PENDING ‚Üí RUNNING ‚Üí COMPLETED (ou FAILED)
```

**Dur√©e** : 2-5 minutes

---

### 4. R√âCUP√âRATION DES R√âSULTATS

#### 4.1 Lister les r√©sultats

```bash
aws s3 ls s3://bucket/output/etape_1/ --recursive --region eu-west-1
```

**R√©sultats attendus** :
```
metadata_202(1118_083045/
‚îú‚îÄ‚îÄ _SUCCESS
‚îî‚îÄ‚îÄ part-00000-xxx.csv

stats_20251118_083045/
‚îú‚îÄ‚îÄ _SUCCESS
‚îî‚îÄ‚îÄ part-00000-xxx.csv
```

#### 4.2 T√©l√©charger les r√©sultats

```bash
mkdir -p results
aws s3 cp s3://bucket/output/etape_1/ ./results/ --recursive --region eu-west-1
```

#### 4.3 Analyse des r√©sultats

**metadata CSV** :
```csv
s3_path,label,filename,split,modification_time,file_size_bytes
s3://.../Training/Apple_Braeburn/image_001_100.jpg,Apple_Braeburn,image_001_100.jpg,Training,2025-01-15 10:23:45,5432
...
```

**stats CSV** :
```csv
split,label,count
Training,Apple_Braeburn,492
Training,Banana,1000
Test,Apple_Braeburn,164
...
```

---

### 5. NETTOYAGE

#### 5.1 Terminaison du cluster

```bash
./scripts/terminate_cluster.sh
# Confirmer avec: oui
```

**Important** : Toujours terminer le cluster pour √©viter des frais !

#### 5.2 Nettoyage complet (optionnel)

```bash
./scripts/cleanup.sh
```

**Options propos√©es** :
1. Terminer le cluster (si actif)
2. Supprimer les donn√©es de sortie S3
3. Supprimer les logs EMR S3
4. Supprimer les fichiers locaux de tracking

---

## üîÑ Workflow alternatifs

### Workflow de d√©veloppement (it√©ratif)

```bash
# 1. Modifier le script PySpark
nano scripts/read_fruits_data.py

# 2. Uploader la nouvelle version
./scripts/upload_scripts.sh

# 3. Soumettre un nouveau step (cluster d√©j√† actif)
./scripts/submit_job.sh

# 4. V√©rifier les r√©sultats
aws s3 ls s3://bucket/output/etape_1/ --recursive

# 5. R√©p√©ter 1-4 jusqu'√† satisfaction
```

### Workflow de debugging

```bash
# 1. Job √©chou√© (FAILED)
aws emr describe-step --cluster-id $(cat cluster_id.txt) --step-id $(cat step_id.txt) --region eu-west-1

# 2. Consulter les logs
aws s3 ls s3://bucket/logs/emr/containers/$(cat cluster_id.txt)/ --recursive

# 3. T√©l√©charger les logs
aws s3 cp s3://bucket/logs/emr/containers/$(cat cluster_id.txt)/ ./logs/ --recursive

# 4. Analyser stderr et stdout
grep -r "ERROR" logs/
grep -r "Exception" logs/

# 5. Corriger le script et re-tester
```

### Workflow de production (automatis√©)

```bash
# Script bash complet pour automatisation
#!/bin/bash
set -e

cd traitement/etape_1

# 1. V√©rification
./scripts/verify_setup.sh || exit 1

# 2. Upload
./scripts/upload_scripts.sh

# 3. Cr√©ation cluster
./scripts/create_cluster.sh

# 4. Attente (polling)
while [ "$(aws emr describe-cluster --cluster-id $(cat cluster_id.txt) --region eu-west-1 --query 'Cluster.Status.State' --output text)" != "WAITING" ]; do
    sleep 30
done

# 5. Soumission job
./scripts/submit_job.sh

# 6. Attente job
while [ "$(aws emr describe-step --cluster-id $(cat cluster_id.txt) --step-id $(cat step_id.txt) --region eu-west-1 --query 'Step.Status.State' --output text)" == "RUNNING" ]; do
    sleep 30
done

# 7. V√©rification succ√®s
STEP_STATE=$(aws emr describe-step --cluster-id $(cat cluster_id.txt) --step-id $(cat step_id.txt) --region eu-west-1 --query 'Step.Status.State' --output text)
if [ "$STEP_STATE" != "COMPLETED" ]; then
    echo "Job failed!"
    exit 1
fi

# 8. T√©l√©chargement r√©sultats
mkdir -p results
aws s3 cp s3://bucket/output/etape_1/ ./results/ --recursive

# 9. Terminaison cluster
./scripts/terminate_cluster.sh

echo "Pipeline completed successfully!"
```

---

## üìä M√©triques et KPIs

### Performance

- **Temps de cr√©ation cluster** : 10-15 min
- **Temps d'ex√©cution job** : 2-5 min (155k images)
- **D√©bit lecture S3** : ~1000 images/sec
- **D√©bit √©criture S3** : ~500 MB/sec

### Co√ªts

- **Co√ªt cluster/heure** : ~0.52‚Ç¨
- **Co√ªt d'un run complet** : ~0.17‚Ç¨ (20 min)
- **Stockage S3** : ~0.02‚Ç¨/mois (1 GB)

### Fiabilit√©

- **Taux de succ√®s** : >95% (si configuration correcte)
- **Causes d'√©chec** :
  - Bootstrap √©choue (5%)
  - Erreur dans le script PySpark (3%)
  - Timeout r√©seau S3 (2%)

---

## üéì Best Practices

1. **Toujours v√©rifier la configuration** avant de cr√©er le cluster (`verify_setup.sh`)
2. **Utiliser l'auto-terminaison** pour √©viter les oublis
3. **Surveiller les co√ªts** r√©guli√®rement (AWS Cost Explorer)
4. **Logs** : Toujours activer et consulter en cas d'√©chec
5. **Versionning** : Garder l'historique des scripts sur Git
6. **Tests locaux** : Tester PySpark localement avant EMR (si possible)
7. **Sauvegardes S3** : Toujours sauvegarder les r√©sultats critiques
8. **Documentation** : Mettre √† jour la doc apr√®s chaque modification

---

## üìö Ressources suppl√©mentaires

- [README.md](../README.md) : Documentation compl√®te
- [QUICKSTART.md](QUICKSTART.md) : Guide de d√©marrage rapide
- [ARCHITECTURE.md](ARCHITECTURE.md) : Architecture technique d√©taill√©e
