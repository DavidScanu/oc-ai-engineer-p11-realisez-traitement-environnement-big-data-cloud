# Architecture technique - Ã‰tape 1

## ğŸ—ï¸ Vue d'ensemble

Cette Ã©tape met en place une architecture Big Data cloud-native sur AWS pour le traitement distribuÃ© d'images.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          AWS Cloud (eu-west-1)                          â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                â”‚
â”‚  â”‚                     â”‚                                                â”‚
â”‚  â”‚    S3 Bucket        â”‚                                                â”‚
â”‚  â”‚  (Data Lake)        â”‚                                                â”‚
â”‚  â”‚                     â”‚                                                â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  â”‚ Input Data   â”‚   â”‚         â”‚     EMR Cluster                â”‚    â”‚
â”‚  â”‚  â”‚ - Training/  â”‚â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                                â”‚    â”‚
â”‚  â”‚  â”‚ - Test/      â”‚   â”‚  Read   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚         â”‚  â”‚   Master Node            â”‚  â”‚    â”‚
â”‚  â”‚                     â”‚         â”‚  â”‚   (m5.xlarge)            â”‚  â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚         â”‚  â”‚                          â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ Scripts      â”‚   â”‚         â”‚  â”‚  - Spark Driver          â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ - Bootstrap  â”‚â”€â”€â”€â”¼â”€â”€â”€â”€â”    â”‚  â”‚  - Resource Manager      â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ - PySpark    â”‚   â”‚    â”‚    â”‚  â”‚  - NameNode (HDFS)       â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â”‚                     â”‚    â”‚    â”‚              â”‚                  â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚    â”‚              â–¼                  â”‚    â”‚
â”‚  â”‚  â”‚ Config       â”‚   â”‚    â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚ - requirementsâ”‚  â”‚    â””â”€â”€â”€â–ºâ”‚  â”‚   Core Nodes (x2)        â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚         â”‚  â”‚   (m5.xlarge each)       â”‚  â”‚    â”‚
â”‚  â”‚                     â”‚         â”‚  â”‚                          â”‚  â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚         â”‚  â”‚  - Spark Executors       â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ Output       â”‚â—„â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚  - DataNode (HDFS)       â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ - metadata/  â”‚   â”‚  Write  â”‚  â”‚  - Task execution        â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ - stats/     â”‚   â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚         â”‚                                â”‚    â”‚
â”‚  â”‚                     â”‚         â”‚  Applications:                 â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚         â”‚  - Hadoop 3.3.x                â”‚    â”‚
â”‚  â”‚  â”‚ Logs         â”‚â—„â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  - Spark 3.5.x                 â”‚    â”‚
â”‚  â”‚  â”‚ - EMR logs   â”‚   â”‚  Write  â”‚  - PySpark 3.5.x               â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚         â”‚                                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   IAM Roles         â”‚         â”‚    VPC / Networking            â”‚    â”‚
â”‚  â”‚                     â”‚         â”‚                                â”‚    â”‚
â”‚  â”‚  - EMR_DefaultRole  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”‚  - Subnet (eu-west-1a)         â”‚    â”‚
â”‚  â”‚  - EMR_EC2_Default  â”‚         â”‚  - Security Groups:            â”‚    â”‚
â”‚  â”‚  - EMR_AutoScaling  â”‚         â”‚    * Master SG                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚    * Slave SG                  â”‚    â”‚
â”‚                                  â”‚  - EC2 Key Pair (SSH)          â”‚    â”‚
â”‚                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Composants techniques

### 1. AWS EMR Cluster

**Version** : EMR 7.11.0

**Configuration** :
- **Master Node** : 1x m5.xlarge (4 vCPU, 16 GB RAM, 32 GB EBS gp3)
- **Core Nodes** : 2x m5.xlarge (4 vCPU, 16 GB RAM, 32 GB EBS gp3)
- **Total** : 3 nÅ“uds, 12 vCPU, 48 GB RAM

**Applications** :
- Hadoop 3.3.6
- Spark 3.5.1
- YARN (Resource Manager)

**Optimisations Spark** :
```yaml
spark.executor.memory: 4g
spark.driver.memory: 4g
spark.executor.memoryOverhead: 1g
spark.sql.execution.arrow.pyspark.enabled: true
```

**Auto-terminaison** : 4 heures d'inactivitÃ©

### 2. AWS S3 (Data Lake)

**Structure** :
```
s3://oc-p11-fruits-david-scanu/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ fruits-360/
â”‚       â”œâ”€â”€ Training/
â”‚       â”‚   â”œâ”€â”€ Apple_Braeburn/
â”‚       â”‚   â”œâ”€â”€ Banana/
â”‚       â”‚   â””â”€â”€ ... (224 classes)
â”‚       â””â”€â”€ Test/
â”‚           â””â”€â”€ ... (224 classes)
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ install_dependencies.sh
â”‚   â””â”€â”€ read_fruits_data.py
â”‚
â”‚
â”œâ”€â”€ output/
â”‚   â””â”€â”€ etape_1/
â”‚       â”œâ”€â”€ metadata_YYYYMMDD_HHMMSS/
â”‚       â”‚   â””â”€â”€ part-00000-xxx.csv
â”‚       â””â”€â”€ stats_YYYYMMDD_HHMMSS/
â”‚           â””â”€â”€ part-00000-xxx.csv
â”‚
â””â”€â”€ logs/
    â””â”€â”€ emr/
        â”œâ”€â”€ j-CLUSTERID/
        â”‚   â”œâ”€â”€ node/
        â”‚   â”œâ”€â”€ containers/
        â”‚   â””â”€â”€ steps/
        â””â”€â”€ ...
```

### 3. IAM Roles et Permissions

**EMR_DefaultRole** (Service Role) :
- `AmazonEMRServicePolicy_v2`
- Permissions : CrÃ©er/gÃ©rer les ressources EMR

**EMR_EC2_DefaultRole** (Instance Profile) :
- `AmazonElasticMapReduceforEC2Role`
- Permissions : AccÃ¨s S3, CloudWatch, EC2

**EMR_AutoScaling_DefaultRole** :
- `AmazonElasticMapReduceforAutoScalingRole`
- Permissions : Auto-scaling du cluster

### 4. VPC et SÃ©curitÃ©

**Subnet** : `subnet-037413c77aa8d5ebb` (eu-west-1a)

**Security Groups** :
- **Master SG** (`sg-0ee431c02c5bc7fc4`) :
  - Ports ouverts : 22 (SSH), 8088 (YARN), 9443 (JupyterHub)
  - Source : IP de l'utilisateur

- **Slave SG** (`sg-03b5c1607e57d5935`) :
  - Communication inter-nÅ“uds
  - Ports Spark : 7077, 4040-4050

**EC2 Key Pair** : `emr-p11-fruits-key-codespace`

## ğŸ“Š Flux de donnÃ©es

### Phase 1 : Bootstrap (au dÃ©marrage du cluster)

```
1. EMR dÃ©marre les instances EC2
   â””â”€â–º 2. TÃ©lÃ©charge install_dependencies.sh depuis S3
        â””â”€â–º 3. Installe les packages Python
            â””â”€â–º 4. Cluster passe Ã  l'Ã©tat WAITING
```

### Phase 2 : ExÃ©cution du job PySpark

```
1. Soumission du step via submit_job.sh
   â””â”€â–º 2. EMR tÃ©lÃ©charge read_fruits_data.py depuis S3
       â””â”€â–º 3. spark-submit lance le job en mode cluster
           â””â”€â–º 4. Driver (Master) coordonne les Executors (Core)
               â”‚
               â”œâ”€â–º 5a. Lecture des images (binaryFile)
               â”‚   â””â”€â–º Spark lit s3://bucket/data/fruits-360/**/*.jpg
               â”‚
               â”œâ”€â–º 5b. Transformation (regex, extraction mÃ©tadonnÃ©es)
               â”‚   â””â”€â–º ParallÃ©lisation sur les 2 Core nodes
               â”‚
               â”œâ”€â–º 5c. AgrÃ©gation (groupBy, count)
               â”‚   â””â”€â–º Calcul distribuÃ© des statistiques
               â”‚
               â””â”€â–º 5d. Ã‰criture des rÃ©sultats
                   â””â”€â–º Sauvegarde CSV sur S3 (coalesce(1))
```

### Phase 3 : Finalisation

```
1. Step terminÃ© (Ã©tat: COMPLETED)
   â””â”€â–º 2. RÃ©sultats disponibles sur S3
       â””â”€â–º 3. Cluster retourne Ã  l'Ã©tat WAITING
           â””â”€â–º 4. Auto-terminaison aprÃ¨s 4h (si inactif)
```

## ğŸ”„ Cycle de vie du cluster

```
CREATE â”€â”€â–º STARTING â”€â”€â–º BOOTSTRAPPING â”€â”€â–º RUNNING â”€â”€â–º WAITING
                                                        â”‚
                                                        â–¼
                                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                               â”‚  Submit Step   â”‚
                                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                                                        â–¼
                                                    RUNNING
                                                        â”‚
                                                        â–¼
                                              Step COMPLETED/FAILED
                                                        â”‚
                                                        â–¼
                                                    WAITING
                                                        â”‚
                                                        â–¼
                                               (4h idle timeout)
                                                        â”‚
                                                        â–¼
                                          TERMINATE â”€â”€â–º TERMINATING â”€â”€â–º TERMINATED
```

## ğŸ’¾ Persistance des donnÃ©es

| Type de donnÃ©es | Emplacement | DurabilitÃ© |
|----------------|-------------|------------|
| Input (images) | S3 | âœ… Permanente |
| Scripts Python | S3 | âœ… Permanente |
| Configuration | S3 | âœ… Permanente |
| Output (CSV) | S3 | âœ… Permanente |
| Logs EMR | S3 | âœ… Permanente (configurable) |
| DonnÃ©es HDFS | Cluster (EBS) | âŒ Perdue Ã  la terminaison |
| Spark cache | MÃ©moire cluster | âŒ Perdue Ã  la terminaison |

**Important** : Toujours sauvegarder les rÃ©sultats sur S3 avant de terminer le cluster !

## ğŸ”’ ConformitÃ© GDPR

âœ… **RÃ©gion Europe** : `eu-west-1` (Irlande)
âœ… **Stockage** : S3 dans `eu-west-1` (pas de rÃ©plication cross-rÃ©gion)
âœ… **Compute** : EMR instances dans `eu-west-1`
âœ… **Logs** : CloudWatch et S3 logs dans `eu-west-1`
âœ… **Pas de transfert hors UE**

## ğŸ’° Estimation des coÃ»ts (eu-west-1)

| Ressource | QuantitÃ© | Tarif unitaire | CoÃ»t/heure |
|-----------|----------|----------------|------------|
| EMR Master m5.xlarge | 1 | ~0.05â‚¬ + 0.12â‚¬ (EMR) | 0.17â‚¬ |
| EMR Core m5.xlarge | 2 | ~0.05â‚¬ + 0.12â‚¬ (EMR) | 0.34â‚¬ |
| EBS gp3 (32 GB) | 3 | ~0.10â‚¬/mois/GB | ~0.01â‚¬ |
| **Total** | | | **~0.52â‚¬/heure** |

**CoÃ»t d'un job typique** (15 min cluster + 5 min job) : ~0.17â‚¬

**S3 Storage** (100k images, ~1 GB) : ~0.02â‚¬/mois

**Remarque** : Les tarifs AWS varient, vÃ©rifier la [calculatrice AWS](https://calculator.aws/).

## ğŸš€ ScalabilitÃ©

### ScalabilitÃ© verticale (instances plus puissantes)

```bash
# Dans config/config.sh
export MASTER_INSTANCE_TYPE="m5.2xlarge"  # 8 vCPU, 32 GB RAM
export CORE_INSTANCE_TYPE="m5.2xlarge"
```

### ScalabilitÃ© horizontale (plus de nÅ“uds)

```bash
# Dans config/config.sh
export CORE_INSTANCE_COUNT="4"  # Au lieu de 2
```

### Auto-scaling (dynamique)

Le cluster peut auto-scaler entre min et max instances selon la charge (dÃ©jÃ  configurÃ© avec `EMR_AutoScaling_DefaultRole`).

## ğŸ“ˆ Performances attendues

| Dataset | Taille | NÅ“uds | DurÃ©e estimÃ©e |
|---------|--------|-------|---------------|
| 155k images (Fruits-360 complet) | ~5 GB | 1M + 2C | 2-5 min |
| 1M images | ~30 GB | 1M + 4C | 10-15 min |
| 10M images | ~300 GB | 1M + 10C | 30-60 min |

**Facteurs influenÃ§ant les performances** :
- Nombre de Core nodes (parallÃ©lisation)
- Type d'instance (vCPU, RAM)
- Bande passante rÃ©seau S3
- Taille des images
- ComplexitÃ© des transformations

## ğŸ”§ Monitoring et Debugging

### Logs EMR

```bash
# Logs principaux
s3://bucket/logs/emr/j-CLUSTERID/
â”œâ”€â”€ node/                    # Logs systÃ¨me des nÅ“uds
â”œâ”€â”€ containers/              # Logs des conteneurs YARN
â””â”€â”€ steps/                   # Logs des steps (jobs)
```

### Spark UI

Accessible pendant l'exÃ©cution du job :
- URL : `http://<master-dns>:4040`
- NÃ©cessite tunnel SSH ou accÃ¨s VPC

### CloudWatch

MÃ©triques automatiques :
- CPU Utilization
- Memory Usage
- HDFS Utilization
- YARN Metrics

## ğŸ¯ Limitations et contraintes

1. **CoÃ»t** : ~0.52â‚¬/heure â†’ Toujours terminer le cluster aprÃ¨s usage
2. **Idle timeout** : 4h d'inactivitÃ© â†’ Cluster auto-terminÃ©
3. **Bande passante S3** : Limitation selon la rÃ©gion et le tier
4. **EBS** : 32 GB par nÅ“ud â†’ Pas de stockage local massif
5. **HDFS Ã©phÃ©mÃ¨re** : DonnÃ©es perdues Ã  la terminaison

## ğŸ”® Ã‰volutions futures (Ã‰tapes 2-4)

- **Ã‰tape 2** : Extraction de features avec TensorFlow (MobileNetV2)
- **Ã‰tape 3** : Broadcast des poids TensorFlow (`sc.broadcast()`)
- **Ã‰tape 4** : PCA distribuÃ© avec MLlib ou PySpark

**Modifications architecturales nÃ©cessaires** :
- Installation de TensorFlow 2.x (dÃ©jÃ  fait)
- Augmentation mÃ©moire executor (pour chargement modÃ¨le)
- Optimisation Pandas UDF pour feature extraction
