#!/usr/bin/env python3
"""
Script PySpark - Ã‰tape 1: Lecture et indexation du dataset Fruits-360

Objectifs:
1. Lire les images depuis S3
2. CrÃ©er un DataFrame avec mÃ©tadonnÃ©es (nom fichier, chemin S3, label)
3. Sauvegarder le rÃ©sultat en CSV sur S3

Ce script valide:
- La lecture de donnÃ©es depuis S3
- L'installation des packages Python (boto3, etc.)
- L'Ã©criture de rÃ©sultats vers S3
"""

import sys
import os
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.functions import input_file_name, regexp_extract, col, lit
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

def main():
    """Fonction principale du script PySpark"""

    # RÃ©cupÃ©rer les arguments
    if len(sys.argv) != 3:
        print("Usage: read_fruits_data.py <s3_input_path> <s3_output_path>")
        print("Example: read_fruits_data.py s3://bucket/data/fruits-360/ s3://bucket/output/etape_1/")
        sys.exit(1)

    s3_input_path = sys.argv[1]
    s3_output_path = sys.argv[2]

    print("=" * 80)
    print("ğŸ P11 - Ã‰TAPE 1: Lecture et indexation du dataset Fruits-360")
    print("=" * 80)
    print(f"ğŸ“¥ Input:  {s3_input_path}")
    print(f"ğŸ“¤ Output: {s3_output_path}")
    print(f"â° DÃ©but:  {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)

    # Initialiser Spark Session
    print("\nğŸ”§ Initialisation de Spark...")
    spark = SparkSession.builder \
        .appName("P11-Etape1-ReadFruitsData") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .getOrCreate()

    # Afficher la configuration Spark
    print(f"âœ… Spark version: {spark.version}")
    print(f"âœ… Spark Master: {spark.sparkContext.master}")
    print(f"âœ… Executor Memory: {spark.sparkContext.getConf().get('spark.executor.memory', 'default')}")
    print(f"âœ… Driver Memory: {spark.sparkContext.getConf().get('spark.driver.memory', 'default')}")

    # Ã‰tape 1: Lire les fichiers images depuis S3
    print("\nğŸ“‚ Ã‰tape 1: Lecture des fichiers images depuis S3...")
    print(f"   Chemin: {s3_input_path}")

    # Lire tous les fichiers JPG (pattern rÃ©cursif)
    # binaryFiles() retourne: (path, content, modification_time)
    df_files = spark.read.format("binaryFile") \
        .option("pathGlobFilter", "*.jpg") \
        .option("recursiveFileLookup", "true") \
        .load(s3_input_path)

    print(f"âœ… Fichiers lus: {df_files.count()} images trouvÃ©es")

    # Ã‰tape 2: Extraire les mÃ©tadonnÃ©es
    print("\nğŸ” Ã‰tape 2: Extraction des mÃ©tadonnÃ©es...")

    # Extraire le nom du fichier et le label depuis le chemin
    # Format: s3://bucket/data/fruits-360/Training/Apple_Braeburn/image_001_100.jpg
    # Label: Apple_Braeburn
    df_metadata = df_files.select(
        col("path").alias("s3_path"),
        regexp_extract(col("path"), r"/([^/]+)/[^/]+\.jpg$", 1).alias("label"),
        regexp_extract(col("path"), r"/([^/]+\.jpg)$", 1).alias("filename"),
        regexp_extract(col("path"), r"/(Training|Test)/", 1).alias("split"),
        col("modificationTime").alias("modification_time"),
        col("length").alias("file_size_bytes")
    )

    # Filtrer les lignes oÃ¹ le label est vide (fichiers mal placÃ©s)
    df_clean = df_metadata.filter(col("label") != "")

    print(f"âœ… MÃ©tadonnÃ©es extraites: {df_clean.count()} images valides")

    # Ã‰tape 3: Statistiques de base
    print("\nğŸ“Š Ã‰tape 3: Calcul des statistiques...")

    # Compter par label et par split
    df_stats = df_clean.groupBy("split", "label").count() \
        .orderBy("split", "label")

    total_training = df_clean.filter(col("split") == "Training").count()
    total_test = df_clean.filter(col("split") == "Test").count()
    total_labels = df_clean.select("label").distinct().count()

    print(f"âœ… Training images: {total_training}")
    print(f"âœ… Test images: {total_test}")
    print(f"âœ… Nombre de classes: {total_labels}")

    # Afficher un Ã©chantillon
    print("\nğŸ“‹ Ã‰chantillon des donnÃ©es:")
    df_clean.select("filename", "label", "split", "file_size_bytes").show(10, truncate=False)

    # Ã‰tape 4: Sauvegarder les rÃ©sultats en CSV
    print("\nğŸ’¾ Ã‰tape 4: Sauvegarde des rÃ©sultats sur S3...")

    # CrÃ©er le timestamp pour le nom du fichier
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_metadata = f"{s3_output_path}/metadata_{timestamp}"
    output_stats = f"{s3_output_path}/stats_{timestamp}"

    print(f"   Metadata: {output_metadata}")
    print(f"   Stats: {output_stats}")

    # Sauvegarder le DataFrame principal (mÃ©tadonnÃ©es)
    df_clean.coalesce(1) \
        .write \
        .mode("overwrite") \
        .option("header", "true") \
        .csv(output_metadata)

    # Sauvegarder les statistiques
    df_stats.coalesce(1) \
        .write \
        .mode("overwrite") \
        .option("header", "true") \
        .csv(output_stats)

    print("âœ… RÃ©sultats sauvegardÃ©s avec succÃ¨s")

    # RÃ©sumÃ© final
    print("\n" + "=" * 80)
    print("âœ… TRAITEMENT TERMINÃ‰ AVEC SUCCÃˆS")
    print("=" * 80)
    print(f"ğŸ“Š Total images: {df_clean.count()}")
    print(f"ğŸ“Š Training: {total_training}")
    print(f"ğŸ“Š Test: {total_test}")
    print(f"ğŸ“Š Classes: {total_labels}")
    print(f"ğŸ“¤ Output: {s3_output_path}")
    print(f"â° Fin: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)

    # ArrÃªter Spark
    spark.stop()

if __name__ == "__main__":
    main()
