# √âtape 2 : Feature Extraction + PCA avec AWS EMR

## üìã Vue d'ensemble

Cette √©tape impl√©mente un pipeline PySpark distribu√© pour:
1. **Extraire des features** des images de fruits avec **MobileNetV2** (Transfer Learning)
2. **R√©duire les dimensions** avec **PCA** (1280 ‚Üí 50 composantes)
3. **Sauvegarder les r√©sultats** sur S3 (Parquet + CSV)

### Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Images    ‚îÇ  S3: s3://bucket/data/raw/Training/
‚îÇ   JPG       ‚îÇ  ~67,000 images de fruits (100x100px)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     AWS EMR Cluster (PySpark)           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  MobileNetV2 (Broadcast Weights)   ‚îÇ ‚îÇ  Features: 1280D
‚îÇ  ‚îÇ  ‚Üí Pandas UDF (distributed)        ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ               ‚ñº                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  PCA (PySpark MLlib)               ‚îÇ ‚îÇ  Reduced: 50D
‚îÇ  ‚îÇ  ‚Üí Variance Analysis               ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
                ‚ñº
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ  S3 Output      ‚îÇ  s3://bucket/process_fruits_data/output/
      ‚îÇ  - features/    ‚îÇ  (1280D: Parquet + CSV)
      ‚îÇ  - pca/         ‚îÇ  (50D: Parquet + CSV)
      ‚îÇ  - metadata/    ‚îÇ  (path, label)
      ‚îÇ  - model_info/  ‚îÇ  (variance, stats)
      ‚îÇ  - errors/      ‚îÇ  (rapport d'erreurs)
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üöÄ D√©marrage rapide

### Pr√©requis

- AWS CLI configur√© avec credentials valides
- Acc√®s S3 au bucket: `oc-p11-fruits-david-scanu`
- Cl√© SSH EMR: `emr-p11-fruits-key-codespace`
- R√©gion AWS: `eu-west-1` (GDPR)

### Workflow complet (5 √©tapes)

```bash
cd traitement/etape_2

# 1. V√©rifier la configuration
./scripts/verify_setup.sh

# 2. Uploader les scripts sur S3
./scripts/upload_scripts.sh

# 3. Cr√©er le cluster EMR (~10-15 min)
./scripts/create_cluster.sh

# 4. Surveiller le d√©marrage (optionnel)
./scripts/monitor_cluster.sh

# 5. Soumettre le job PySpark
./scripts/submit_job.sh
# ‚Üí Choisir le mode: mini (300 images) / apples (~6,400) / full (~67,000)

# 6. T√©l√©charger les r√©sultats
./scripts/download_results.sh

# 7. Arr√™ter le cluster (IMPORTANT pour les co√ªts !)
./scripts/terminate_cluster.sh
```

---

## ‚öôÔ∏è Configuration

### Fichier: [config/config.sh](../config/config.sh)

Variables cl√©s:

```bash
# S3
S3_BUCKET="oc-p11-fruits-david-scanu"
S3_DATA_INPUT="s3://${S3_BUCKET}/data/raw/"
S3_DATA_OUTPUT="s3://${S3_BUCKET}/process_fruits_data/output/"

# EMR Cluster
CLUSTER_NAME="p11-fruits-etape2"
MASTER_INSTANCE_TYPE="m5.2xlarge"   # 8 vCPU, 32 GB RAM
CORE_INSTANCE_TYPE="m5.2xlarge"
CORE_INSTANCE_COUNT="2"

# Spark Memory
SPARK_EXECUTOR_MEMORY="8g"
SPARK_DRIVER_MEMORY="8g"

# PCA
PCA_COMPONENTS="50"
DEFAULT_MODE="mini"
MINI_IMAGES_COUNT="300"
```

---

## üìÇ Structure des outputs S3

```
s3://oc-p11-fruits-david-scanu/process_fruits_data/output/
‚îú‚îÄ‚îÄ features/
‚îÇ   ‚îú‚îÄ‚îÄ parquet/features_YYYYMMDD_HHMMSS/    # Features 1280D (Parquet)
‚îÇ   ‚îî‚îÄ‚îÄ csv/features_YYYYMMDD_HHMMSS/        # Features 1280D (CSV)
‚îú‚îÄ‚îÄ pca/
‚îÇ   ‚îú‚îÄ‚îÄ parquet/pca_YYYYMMDD_HHMMSS/         # PCA 50D (Parquet)
‚îÇ   ‚îî‚îÄ‚îÄ csv/pca_YYYYMMDD_HHMMSS/             # PCA 50D (CSV)
‚îú‚îÄ‚îÄ metadata/metadata_YYYYMMDD_HHMMSS/       # path, label
‚îú‚îÄ‚îÄ model_info/
‚îÇ   ‚îú‚îÄ‚îÄ model_info_YYYYMMDD_HHMMSS/          # JSON: variance, stats
‚îÇ   ‚îî‚îÄ‚îÄ variance_YYYYMMDD_HHMMSS/            # CSV: variance par composante
‚îî‚îÄ‚îÄ errors/errors_YYYYMMDD_HHMMSS/           # Rapport d'erreurs (si pr√©sent)
```

---

## üéØ Modes de traitement

| Mode   | Images     | Classes       | Dur√©e estim√©e | Usage                      |
|--------|------------|---------------|---------------|----------------------------|
| **mini**   | 300        | Pommes (3-4)  | 2-5 min       | Tests rapides, validation  |
| **apples** | ~6,400     | Pommes (~29)  | 15-30 min     | Tests interm√©diaires       |
| **full**   | ~67,000    | Tous (224)    | 2-3 heures    | Production compl√®te        |

---

## üí∞ Co√ªts AWS

### Cluster EMR (m5.2xlarge)

- **1 Master** : ~0.384 $/h
- **2 Core** : ~0.768 $/h
- **Total** : ~1.15 $/h (‚âà 0.80-1.00 ‚Ç¨/h)

### Auto-terminaison

- **Timeout** : 4 heures d'inactivit√©
- **Important** : Toujours terminer manuellement apr√®s usage !

```bash
./scripts/terminate_cluster.sh
```

---

## üìä Pipeline PySpark

### 1. Chargement des images

```python
df_images = spark.read.format("binaryFile").load(s3_path)
```

### 2. Extraction des features (MobileNetV2)

```python
# Broadcast des poids du mod√®le
model_weights = model.get_weights()
broadcast_weights = sc.broadcast(model_weights)

# Pandas UDF pour extraction distribu√©e
@pandas_udf(ArrayType(FloatType()))
def extract_features_udf(content_series: pd.Series) -> pd.Series:
    local_model = MobileNetV2(weights=None, include_top=False, pooling='avg')
    local_model.set_weights(broadcast_weights.value)
    # Process images...
    return features
```

### 3. PCA (PySpark MLlib)

```python
from pyspark.ml.feature import PCA

pca = PCA(k=50, inputCol="features_vector", outputCol="pca_features")
pca_model = pca.fit(df_features)
df_pca = pca_model.transform(df_features)
```

### 4. Sauvegarde multi-format

- **Parquet** : Format optimis√© pour big data
- **CSV** : Lisibilit√© et compatibilit√©

---

## üîç Scripts disponibles

| Script                            | Description                                      |
|-----------------------------------|--------------------------------------------------|
| `verify_setup.sh`                 | V√©rifications pr√©-vol (AWS, S3, IAM, SSH)       |
| `upload_scripts.sh`               | Upload des scripts sur S3                        |
| `create_cluster.sh`               | Cr√©ation du cluster EMR                          |
| `monitor_cluster.sh`              | Surveillance du d√©marrage (STARTING ‚Üí WAITING)   |
| `submit_job.sh`                   | Soumission du job PySpark (choix du mode)        |
| `download_results.sh`             | T√©l√©chargement des r√©sultats depuis S3           |
| `download_and_inspect_logs.sh`    | T√©l√©chargement et inspection des logs EMR        |
| `terminate_cluster.sh`            | Arr√™t du cluster                                 |
| `cleanup.sh`                      | Nettoyage complet (cluster + S3 + fichiers)      |

---

## üêõ Troubleshooting

### Le cluster ne d√©marre pas

```bash
# V√©rifier l'√©tat
aws emr describe-cluster --cluster-id j-XXXXXXXXXXXX --region eu-west-1

# V√©rifier les logs
aws s3 ls s3://oc-p11-fruits-david-scanu/process_fruits_data/logs/emr/
```

### Le job √©choue

```bash
# T√©l√©charger et inspecter les logs
./scripts/download_and_inspect_logs.sh

# V√©rifier stderr
cat logs/stderr | grep -i "error"
```

### Probl√®mes TensorFlow

```bash
# V√©rifier l'installation dans les logs bootstrap
aws s3 ls s3://bucket/process_fruits_data/logs/emr/j-XXXX/node/*/bootstrap-actions/
```

### Co√ªts √©lev√©s

```bash
# V√©rifier les instances EC2 actives
aws ec2 describe-instances --region eu-west-1 \
  --filters "Name=instance-state-name,Values=running" \
  --output table

# Terminer le cluster imm√©diatement
./scripts/terminate_cluster.sh
```

---

## üìö Documentation compl√®te

- **[WORKFLOW.md](WORKFLOW.md)** : Workflow d√©taill√© √©tape par √©tape
- **[ARCHITECTURE.md](ARCHITECTURE.md)** : Architecture technique compl√®te

---

## üéì Points cl√©s du projet

### ‚úÖ Optimisations Big Data

1. **Broadcast des poids** : Distribution efficace du mod√®le TensorFlow
2. **Pandas UDF** : Traitement distribu√© avec Arrow serialization
3. **PySpark MLlib** : PCA scalable sur cluster
4. **Multi-format** : Parquet (performance) + CSV (lisibilit√©)

### ‚úÖ Conformit√© GDPR

- R√©gion AWS: `eu-west-1` (Irlande)
- Toutes les donn√©es restent en Europe

### ‚úÖ Gestion des co√ªts

- Auto-terminaison (4h idle)
- Alertes de co√ªts
- Scripts de nettoyage

---

## üìû Support

### V√©rifier la configuration

```bash
./scripts/verify_setup.sh
```

### Voir la configuration actuelle

```bash
source config/config.sh
show_config
```

### Logs AWS EMR

```bash
# Console web
https://eu-west-1.console.aws.amazon.com/emr/home?region=eu-west-1

# Logs S3
aws s3 ls s3://oc-p11-fruits-david-scanu/process_fruits_data/logs/emr/ --recursive
```

---

## üèÜ R√©sultats attendus

Apr√®s ex√©cution compl√®te (mode `full`):

- **Features brutes** : ~67,000 images √ó 1280 dimensions
- **Features PCA** : ~67,000 images √ó 50 dimensions
- **Variance expliqu√©e** : Information dans `model_info/`
- **Taux d'erreur** : < 1% (images corrompues/invalides)

---

**Projet** : OpenClassrooms AI Engineer P11
**Environnement** : AWS EMR 7.11.0, Spark 3.x, TensorFlow 2.16.1
**Dataset** : Fruits-360 (Kaggle)
