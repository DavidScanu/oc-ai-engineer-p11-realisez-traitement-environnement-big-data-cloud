{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P11 - Traitement Big Data Cloud : Classification de Fruits\n",
    "\n",
    "**Projet** : OpenClassrooms AI Engineer P11  \n",
    "**Environnement** : AWS EMR Notebook  \n",
    "**Dataset** : Fruits-360 (Kaggle)  \n",
    "\n",
    "## Pipeline PySpark\n",
    "\n",
    "1. Chargement des images depuis S3\n",
    "2. Extraction de features avec TensorFlow MobileNetV2\n",
    "3. Broadcast des poids du mod√®le (optimisation distribu√©e)\n",
    "4. R√©duction de dimension avec PCA\n",
    "5. Sauvegarde des r√©sultats sur S3\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Configuration EMR\n",
    "\n",
    "- ‚úÖ SparkSession d√©j√† disponible (variable `spark`)\n",
    "- ‚úÖ Acc√®s S3 pr√©configur√©\n",
    "- ‚úÖ Cluster multi-workers pour traitement distribu√©\n",
    "\n",
    "**Bucket S3** : `oc-p11-fruits-david-scanu`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuration et Setup\n",
    "\n",
    "### 1.1 D√©sactivation des warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# D√©sactiver les warnings TensorFlow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# D√©sactiver les warnings Python\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurer le logging PySpark\n",
    "logging.getLogger('py4j').setLevel(logging.ERROR)\n",
    "logging.getLogger('pyspark').setLevel(logging.ERROR)\n",
    "\n",
    "print(\"‚úÖ Configuration des warnings appliqu√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Installation de TensorFlow (si n√©cessaire)\n",
    "\n",
    "Sur EMR Notebooks, TensorFlow n'est pas toujours pr√©-install√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier si TensorFlow est install√©\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(f\"‚úÖ TensorFlow d√©j√† install√©: {tf.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚è≥ Installation de TensorFlow...\")\n",
    "    !pip install tensorflow==2.16.1 -q\n",
    "    print(\"‚úÖ TensorFlow install√©\")\n",
    "    print(\"‚ö†Ô∏è  IMPORTANT: Red√©marrez le kernel (Kernel ‚Üí Restart Kernel)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark imports\n",
    "from pyspark.sql.functions import col, pandas_udf, element_at, split, udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "# TensorFlow imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# Autres imports\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import time\n",
    "\n",
    "print(\"‚úÖ Imports r√©ussis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Configuration des chemins S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION S3\n",
    "# ============================================================\n",
    "\n",
    "BUCKET_NAME = \"oc-p11-fruits-david-scanu\"\n",
    "\n",
    "# Chemins S3\n",
    "S3_INPUT_PATH = f\"s3://{BUCKET_NAME}/data/raw/Training/\"\n",
    "S3_FEATURES_OUTPUT = f\"s3://{BUCKET_NAME}/data/features/\"\n",
    "S3_PCA_OUTPUT = f\"s3://{BUCKET_NAME}/data/pca/\"\n",
    "\n",
    "print(f\"üì¶ Bucket S3: {BUCKET_NAME}\")\n",
    "print(f\"üì• Input: {S3_INPUT_PATH}\")\n",
    "print(f\"üì§ Features output: {S3_FEATURES_OUTPUT}\")\n",
    "print(f\"üì§ PCA output: {S3_PCA_OUTPUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Configuration de Spark\n",
    "\n",
    "Sur EMR Notebooks, la variable `spark` est d√©j√† disponible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du niveau de log\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# R√©cup√©rer le SparkContext pour le broadcast\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(f\"‚úÖ SparkSession EMR configur√©e\")\n",
    "print(f\"   Version Spark: {spark.version}\")\n",
    "print(f\"   Master: {spark.sparkContext.master}\")\n",
    "print(f\"   App Name: {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Chargement des Donn√©es depuis S3\n",
    "\n",
    "### 2.1 Modes de chargement\n",
    "\n",
    "| Mode | Nombre d'images | Usage |\n",
    "|------|-----------------|-------|\n",
    "| **MINI** | 100-500 | Tests rapides |\n",
    "| **APPLES** | ~6,400 | Validation |\n",
    "| **FULL** | ~67,000 | Production compl√®te |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION DU MODE DE CHARGEMENT\n",
    "# ============================================================\n",
    "\n",
    "# D√©commenter UNE SEULE option :\n",
    "\n",
    "# MODE 1: MINI TEST (100 images) - RECOMMAND√â pour d√©buter\n",
    "TEST_MODE = \"mini\"\n",
    "MAX_IMAGES = 100\n",
    "\n",
    "# MODE 2: SUBSET POMMES (~6,400 images)\n",
    "# TEST_MODE = \"apples\"\n",
    "\n",
    "# MODE 3: DATASET COMPLET (~67,000 images)\n",
    "# TEST_MODE = \"full\"\n",
    "\n",
    "# ============================================================\n",
    "# CHARGEMENT DES IMAGES\n",
    "# ============================================================\n",
    "\n",
    "if TEST_MODE == \"mini\":\n",
    "    print(f\"üîç Mode: MINI TEST ({MAX_IMAGES} images)\")\n",
    "    image_path = f\"{S3_INPUT_PATH}Apple*/*.jpg\"\n",
    "    df_images = spark.read.format(\"binaryFile\").load(image_path).limit(MAX_IMAGES)\n",
    "    \n",
    "elif TEST_MODE == \"apples\":\n",
    "    print(f\"üîç Mode: SUBSET POMMES (~6,400 images)\")\n",
    "    image_path = f\"{S3_INPUT_PATH}Apple*/*.jpg\"\n",
    "    df_images = spark.read.format(\"binaryFile\").load(image_path)\n",
    "    \n",
    "elif TEST_MODE == \"full\":\n",
    "    print(f\"üîç Mode: DATASET COMPLET (~67,000 images)\")\n",
    "    image_path = f\"{S3_INPUT_PATH}*/*.jpg\"\n",
    "    df_images = spark.read.format(\"binaryFile\").load(image_path)\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Mode inconnu: {TEST_MODE}\")\n",
    "\n",
    "# Afficher le r√©sultat\n",
    "num_images = df_images.count()\n",
    "print(f\"‚úÖ {num_images} images charg√©es depuis S3\")\n",
    "print(f\"\\nüëÄ Aper√ßu:\")\n",
    "df_images.show(5, truncate=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Extraction des labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire le label depuis le chemin\n",
    "# s3://bucket/data/raw/Training/Apple Braeburn/image.jpg -> Apple Braeburn\n",
    "\n",
    "df_with_labels = df_images.withColumn(\n",
    "    \"label\",\n",
    "    element_at(split(col(\"path\"), \"/\"), -2)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Labels extraits\")\n",
    "print(f\"\\nüìä Distribution des classes:\")\n",
    "df_with_labels.groupBy(\"label\").count().orderBy(\"label\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Extraction de Features avec MobileNetV2\n",
    "\n",
    "### 3.1 Chargement du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger MobileNetV2 sans la couche de classification\n",
    "model = MobileNetV2(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    pooling='avg'\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Mod√®le MobileNetV2 charg√©\")\n",
    "print(f\"   Input shape: {model.input_shape}\")\n",
    "print(f\"   Output shape: {model.output_shape}\")\n",
    "print(f\"   Dimension des features: {model.output_shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Broadcast des poids du mod√®le\n",
    "\n",
    "**Optimisation critique** : Le broadcast distribue les poids une seule fois √† tous les workers, √©vitant des t√©l√©chargements r√©p√©t√©s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire les poids\n",
    "model_weights = model.get_weights()\n",
    "\n",
    "print(f\"üì¶ Nombre de tenseurs de poids: {len(model_weights)}\")\n",
    "print(f\"üì¶ Taille en m√©moire: {sum([w.nbytes for w in model_weights]) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Broadcaster les poids\n",
    "broadcast_weights = sc.broadcast(model_weights)\n",
    "\n",
    "print(\"‚úÖ Poids broadcast√©s √† tous les workers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 D√©finition de la Pandas UDF\n",
    "\n",
    "La Pandas UDF permet d'appliquer TensorFlow de mani√®re distribu√©e sur le cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sch√©ma de sortie : array de 1280 floats\n",
    "features_schema = ArrayType(FloatType())\n",
    "\n",
    "@pandas_udf(features_schema)\n",
    "def extract_features_udf(content_series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Extrait les features avec MobileNetV2.\n",
    "    Ex√©cut√© sur chaque worker Spark.\n",
    "    \"\"\"\n",
    "    # Reconstruire le mod√®le dans le worker\n",
    "    local_model = MobileNetV2(\n",
    "        weights=None,\n",
    "        include_top=False,\n",
    "        pooling='avg'\n",
    "    )\n",
    "    \n",
    "    # Charger les poids broadcast√©s\n",
    "    local_model.set_weights(broadcast_weights.value)\n",
    "    \n",
    "    def process_image(content):\n",
    "        try:\n",
    "            # Charger l'image\n",
    "            img = Image.open(io.BytesIO(content))\n",
    "            \n",
    "            # Convertir en RGB\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            \n",
    "            # Redimensionner (224x224)\n",
    "            img = img.resize((224, 224))\n",
    "            \n",
    "            # Convertir en array\n",
    "            img_array = img_to_array(img)\n",
    "            img_array = np.expand_dims(img_array, axis=0)\n",
    "            img_array = preprocess_input(img_array)\n",
    "            \n",
    "            # Extraire les features\n",
    "            features = local_model.predict(img_array, verbose=0)\n",
    "            \n",
    "            return features[0].tolist()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur: {e}\")\n",
    "            return None\n",
    "    \n",
    "    return content_series.apply(process_image)\n",
    "\n",
    "print(\"‚úÖ Pandas UDF d√©finie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Extraction des features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚è≥ Extraction des features...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Appliquer l'extraction\n",
    "df_features = df_with_labels.withColumn(\n",
    "    \"features\",\n",
    "    extract_features_udf(col(\"content\"))\n",
    ")\n",
    "\n",
    "# Filtrer les erreurs\n",
    "df_features = df_features.filter(col(\"features\").isNotNull())\n",
    "\n",
    "# Cache pour r√©utilisation\n",
    "df_features.cache()\n",
    "count = df_features.count()\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚úÖ Features extraites pour {count} images\")\n",
    "print(f\"   Temps d'ex√©cution: {elapsed_time:.2f} secondes\")\n",
    "print(f\"   Vitesse: {count / elapsed_time:.2f} images/seconde\")\n",
    "df_features.select(\"label\", \"features\").show(5, truncate=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. R√©duction de Dimension avec PCA\n",
    "\n",
    "### 4.1 Pr√©paration des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir array ‚Üí vecteur dense pour PCA\n",
    "array_to_vector = udf(lambda a: Vectors.dense(a), VectorUDT())\n",
    "\n",
    "df_for_pca = df_features.withColumn(\n",
    "    \"features_vector\",\n",
    "    array_to_vector(col(\"features\"))\n",
    ")\n",
    "\n",
    "df_for_pca.cache()\n",
    "count = df_for_pca.count()\n",
    "\n",
    "print(f\"‚úÖ {count} vecteurs pr√©par√©s pour PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Application de la PCA\n",
    "\n",
    "R√©duction de 1280 dimensions ‚Üí 200 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_COMPONENTS = 200\n",
    "\n",
    "print(f\"‚è≥ Application de la PCA (1280 ‚Üí {K_COMPONENTS} dimensions)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Cr√©er et entra√Æner le mod√®le PCA\n",
    "pca = PCA(\n",
    "    k=K_COMPONENTS,\n",
    "    inputCol=\"features_vector\",\n",
    "    outputCol=\"pca_features\"\n",
    ")\n",
    "\n",
    "pca_model = pca.fit(df_for_pca)\n",
    "\n",
    "# Appliquer la transformation\n",
    "df_pca = pca_model.transform(df_for_pca)\n",
    "\n",
    "df_pca.cache()\n",
    "count = df_pca.count()\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚úÖ PCA appliqu√©e avec succ√®s !\")\n",
    "print(f\"   Dimensions: 1280 ‚Üí {K_COMPONENTS}\")\n",
    "print(f\"   Images: {count}\")\n",
    "print(f\"   Temps: {elapsed_time:.2f} secondes\")\n",
    "\n",
    "df_pca.select(\"label\", \"pca_features\").show(5, truncate=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Analyse de la variance expliqu√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance expliqu√©e\n",
    "explained_variance = pca_model.explainedVariance\n",
    "\n",
    "print(f\"üìä Variance expliqu√©e:\")\n",
    "print(f\"   Total: {sum(explained_variance):.4f}\")\n",
    "print(f\"   Top 10 composantes:\")\n",
    "for i, var in enumerate(explained_variance[:10]):\n",
    "    print(f\"   PC{i+1}: {var:.6f}\")\n",
    "\n",
    "# Variance cumul√©e\n",
    "cumsum_variance = np.cumsum(explained_variance)\n",
    "print(f\"\\n   Variance cumul√©e (50 premi√®res composantes): {cumsum_variance[49]:.4f}\")\n",
    "print(f\"   Variance cumul√©e (toutes {K_COMPONENTS} composantes): {cumsum_variance[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Sauvegarde des R√©sultats sur S3\n",
    "\n",
    "### 5.1 Sauvegarde en Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©lectionner les colonnes pertinentes\n",
    "df_final = df_pca.select(\"path\", \"label\", \"pca_features\")\n",
    "\n",
    "# Sauvegarder en Parquet sur S3\n",
    "pca_output_path = S3_PCA_OUTPUT + \"pca_results\"\n",
    "\n",
    "print(f\"‚è≥ Sauvegarde sur S3...\")\n",
    "df_final.write.mode(\"overwrite\").parquet(pca_output_path)\n",
    "\n",
    "print(f\"‚úÖ R√©sultats PCA sauvegard√©s: {pca_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Sauvegarde en CSV (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir vecteur ‚Üí string pour CSV\n",
    "def vector_to_string(v):\n",
    "    if v is None:\n",
    "        return None\n",
    "    return \",\".join([str(float(x)) for x in v.toArray()])\n",
    "\n",
    "vector_to_string_udf = udf(vector_to_string, StringType())\n",
    "\n",
    "df_final_csv = df_final.withColumn(\n",
    "    \"pca_features_string\",\n",
    "    vector_to_string_udf(col(\"pca_features\"))\n",
    ").select(\"path\", \"label\", \"pca_features_string\")\n",
    "\n",
    "# Sauvegarder en CSV\n",
    "csv_output_path = S3_PCA_OUTPUT + \"pca_results_csv\"\n",
    "\n",
    "print(f\"‚è≥ Sauvegarde CSV sur S3...\")\n",
    "df_final_csv.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_output_path)\n",
    "\n",
    "print(f\"‚úÖ R√©sultats CSV sauvegard√©s: {csv_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. V√©rification et Nettoyage\n",
    "\n",
    "### 6.1 V√©rification des fichiers sur S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lister les fichiers cr√©√©s\n",
    "print(\"üìÅ Fichiers Parquet:\")\n",
    "!aws s3 ls s3://{BUCKET_NAME}/data/pca/pca_results/ --human-readable\n",
    "\n",
    "print(\"\\nüìÅ Fichiers CSV:\")\n",
    "!aws s3 ls s3://{BUCKET_NAME}/data/pca/pca_results_csv/ --human-readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Lib√©ration des ressources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpersist les DataFrames\n",
    "df_features.unpersist()\n",
    "df_for_pca.unpersist()\n",
    "df_pca.unpersist()\n",
    "\n",
    "# D√©truire le broadcast\n",
    "broadcast_weights.unpersist()\n",
    "\n",
    "print(\"‚úÖ Ressources lib√©r√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä R√©sum√© de l'ex√©cution\n",
    "\n",
    "**Pipeline complet ex√©cut√© :**\n",
    "\n",
    "1. ‚úÖ Chargement des images depuis S3\n",
    "2. ‚úÖ Extraction des labels\n",
    "3. ‚úÖ Extraction de features avec MobileNetV2 (broadcast optimis√©)\n",
    "4. ‚úÖ R√©duction PCA (1280 ‚Üí 200 dimensions)\n",
    "5. ‚úÖ Sauvegarde des r√©sultats sur S3 (Parquet + CSV)\n",
    "\n",
    "**Prochaines √©tapes :**\n",
    "\n",
    "- Ex√©cuter sur le dataset complet (~67,000 images)\n",
    "- Analyser les r√©sultats\n",
    "- Optimiser le nombre de composantes PCA si n√©cessaire\n",
    "- Arr√™ter le cluster EMR pour √©viter les co√ªts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
