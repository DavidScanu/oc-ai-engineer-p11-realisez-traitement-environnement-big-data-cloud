# Plan d'Action - Projet Big Data Fruits

## Vue d'ensemble

Ce document d√©crit le plan d'action d√©taill√© pour compl√©ter le projet de traitement Big Data sur le cloud pour la classification de fruits.

## √âtat des lieux

### Travail de l'alternant (Compl√©t√©)

L'alternant a produit un notebook complet (`P8_Notebook_Linux_EMR_PySpark_V1.0.ipynb`) qui contient :

‚úÖ **Infrastructure Cloud AWS**
- Configuration d√©taill√©e d'AWS EMR (instances, s√©curit√©, bootstrap)
- Configuration S3 pour le stockage des donn√©es
- Mise en place du tunnel SSH et acc√®s JupyterHub
- Documentation step-by-step pour instancier/arr√™ter/cloner le cluster

‚úÖ **Traitement PySpark fonctionnel**
- Chargement d'images depuis S3 avec PySpark
- Extraction de features avec TensorFlow MobileNetV2 (Transfer Learning)
- Utilisation de Pandas UDF pour le traitement distribu√©
- Sauvegarde des r√©sultats sur S3

‚úÖ **Technologies ma√Ætris√©es**
- PySpark 3.1.1 avec Apache Arrow
- TensorFlow 2.4.1 (MobileNetV2)
- PIL pour le preprocessing d'images
- AWS EMR 6.3.0 + S3

**üìå Note** : L'infrastructure a √©volu√© depuis le travail de l'alternant
- EMR actuel : 7.10.0 (vs 6.3.0)
- Spark actuel : 3.5.5 (vs 3.1.1)
- TensorFlow actuel : 2.16.1 (vs 2.4.1)
- Nouveau connecteur S3A par d√©faut (vs EMRFS)

### Composants manquants (√Ä impl√©menter)

‚ùå **1. Broadcast des poids du mod√®le TensorFlow**
- Le mod√®le MobileNetV2 est actuellement recharg√© sur chaque worker
- N√©cessite l'impl√©mentation de `sc.broadcast()` pour distribuer les weights une seule fois
- Am√©liore consid√©rablement les performances et r√©duit la consommation m√©moire

‚ùå **2. R√©duction de dimension PCA en PySpark**
- Actuellement, seule l'extraction de features est r√©alis√©e
- N√©cessite l'ajout d'une √©tape PCA avec `pyspark.ml.feature.PCA`
- Output attendu : matrice CSV avec dimensions r√©duites

---

## Phase 1 : Analyse et Pr√©paration (Local)

### 1.1 √âtude du notebook de l'alternant ‚úÖ TERMIN√â

**Objectif** : Comprendre l'architecture et le code existant

**Actions** :
- [x] Lire et analyser le notebook complet
- [x] Identifier les points d'insertion pour les nouveaux composants
- [x] Comprendre la structure des donn√©es (schema PySpark)
- [x] Analyser l'utilisation de Pandas UDF

**Livrables** ‚úÖ :
- ‚úÖ Notes dans CLAUDE.md (section "Intern's Work Analysis")
- ‚úÖ Notebook alternant import√© dans `notebooks/alternant/`
- ‚úÖ 47 images de documentation EMR import√©es

**Date de compl√©tion** : 24 octobre 2025

### 1.2 T√©l√©chargement et exploration du dataset ‚úÖ TERMIN√â

**Objectif** : Obtenir et analyser le jeu de donn√©es Fruits-360

**Actions** :
- [x] T√©l√©charger le dataset (lien direct S3)
- [x] Extraire et explorer la structure des dossiers
- [x] V√©rifier la qualit√© des images (format, dimensions)
- [x] Analyser la distribution des classes

**Livrables** ‚úÖ :
- ‚úÖ Dataset local dans `data/raw/fruits-360_dataset/` (67,692 images Training, 131 classes)
- ‚úÖ Dataset original-size dans `data/raw/fruits-360-original-size/`
- ‚úÖ Documentation compl√®te dans `documentation/DATASET_INFO.md`
- ‚úÖ ZIP supprim√© pour √©conomiser l'espace (1.3GB)
- ‚úÖ Configuration .gitignore pour exclure les donn√©es volumineuses

**Statistiques** :
- Training: 67,692 images, 131 classes
- Test: 22,688 images
- Format: JPG 100x100 pixels
- Taille totale: ~1.5 GB

**Date de compl√©tion** : 24 octobre 2025

### 1.3 Setup environnement local PySpark ‚úÖ TERMIN√â

**Objectif** : Reproduire l'environnement local pour tester le code

**Actions** :
- [x] V√©rifier Java JDK (Java 11 ‚úÖ compatible)
- [x] Cr√©er requirements.txt avec PySpark 3.5.0
- [x] D√©finir TensorFlow 2.16.1
- [x] Lister toutes les d√©pendances (PIL, pandas, numpy, pyarrow)
- [x] Configurer VSCode pour le projet

**Livrables** ‚úÖ :
- ‚úÖ `requirements.txt` cr√©√© avec toutes les d√©pendances
- ‚úÖ `.vscode/settings.json` configur√© (exclusion data/, config Python/Jupyter)
- ‚úÖ Java 11.0.27 v√©rifi√© et compatible
- ‚úÖ Structure de projet compl√®te

**Date de compl√©tion** : 24 octobre 2025

---

## Phase 2 : D√©veloppement Local

### 2.1 Impl√©menter le broadcast des poids TensorFlow ‚úÖ IMPL√âMENT√â (√Ä TESTER)

**Objectif** : Optimiser la distribution du mod√®le sur les workers

**Statut** : ‚úÖ Code impl√©ment√© dans le notebook, pr√™t pour tests locaux

**Contexte technique** :
Sans broadcast, chaque worker recharge le mod√®le MobileNetV2 (plusieurs MB), ce qui :
- Augmente la consommation m√©moire
- Ralentit les traitements
- G√©n√®re du trafic r√©seau inutile

**Actions** :
- [ ] Extraire les weights du mod√®le MobileNetV2
- [ ] Impl√©menter `broadcast_weights = sc.broadcast(model.get_weights())`
- [ ] Modifier la Pandas UDF pour utiliser les weights broadcast√©s
- [ ] Reconstruire le mod√®le dans chaque worker avec les weights
- [ ] Tester en local avec un subset du dataset

**Code pattern attendu** :
```python
# Dans la cellule de pr√©paration du mod√®le
model = MobileNetV2(weights='imagenet', include_top=False, pooling='avg')
broadcast_weights = sc.broadcast(model.get_weights())

# Dans la Pandas UDF
def extract_features(content_series):
    model = MobileNetV2(weights=None, include_top=False, pooling='avg')
    model.set_weights(broadcast_weights.value)
    # ... reste du code
```

**Livrables** ‚úÖ :
- ‚úÖ Code impl√©ment√© dans `notebooks/p11-david-scanu-local-development.ipynb`
- ‚úÖ Extraction des poids: `model_weights = model.get_weights()`
- ‚úÖ Broadcast: `broadcast_weights = sc.broadcast(model_weights)`
- ‚úÖ Reconstruction dans workers: `local_model.set_weights(broadcast_weights.value)`
- ‚è≥ Tests de performance √† r√©aliser

**Date d'impl√©mentation** : 24 octobre 2025

### 2.2 Impl√©menter la r√©duction PCA en PySpark ‚úÖ IMPL√âMENT√â (√Ä TESTER)

**Objectif** : Ajouter une √©tape de dimensionnalit√© reduction apr√®s l'extraction de features

**Statut** : ‚úÖ Code impl√©ment√© dans le notebook, pr√™t pour tests locaux

**Contexte technique** :
MobileNetV2 (sans top, avec pooling='avg') produit des features de dimension 1280.
La PCA permettra de r√©duire cette dimension tout en conservant la variance significative.

**Actions** :
- [ ] Charger les features extraites (output de l'√©tape pr√©c√©dente)
- [ ] Assembler les features en un vecteur avec `VectorAssembler`
- [ ] Appliquer `pyspark.ml.feature.PCA` sur les features
- [ ] Configurer le nombre de composantes (ex: 100, 200, ou variance expliqu√©e)
- [ ] Sauvegarder le r√©sultat en CSV sur S3
- [ ] Tester en local avec un subset du dataset

**Code pattern attendu** :
```python
from pyspark.ml.feature import PCA, VectorAssembler

# Assembler les colonnes de features en un vecteur
assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
df_features = assembler.transform(df_extracted)

# Appliquer PCA
pca = PCA(k=100, inputCol="features", outputCol="pca_features")
model_pca = pca.fit(df_features)
df_pca = model_pca.transform(df_features)

# Sauvegarder
df_pca.select("image_path", "label", "pca_features").write.csv("s3://bucket/pca_output/")
```

**Livrables** ‚úÖ :
- ‚úÖ Code PCA impl√©ment√© avec `pyspark.ml.feature.PCA`
- ‚úÖ Configuration k=200 composantes (param√©trable)
- ‚úÖ Conversion array ‚Üí vecteur dense avec UDF
- ‚úÖ Analyse de variance expliqu√©e pr√©par√©e
- ‚úÖ Sauvegarde en Parquet et CSV
- ‚è≥ Tests et validation √† r√©aliser

**Date d'impl√©mentation** : 24 octobre 2025

### 2.3 Int√©gration et tests locaux ‚è≥ EN ATTENTE

**Objectif** : Valider le pipeline complet en local

**Statut** : ‚è≥ Pr√™t pour tests - n√©cessite installation d√©pendances et ex√©cution notebook

**Actions** :
- [ ] Cr√©er un notebook consolid√© avec toutes les modifications
- [ ] Tester sur un subset du dataset (ex: 1000 images)
- [ ] V√©rifier les outputs √† chaque √©tape
- [ ] Mesurer les temps d'ex√©cution
- [ ] Documenter le code avec des commentaires clairs

**Livrables** :
- Notebook PySpark fonctionnel en local
- Documentation des r√©sultats de tests

---

## Phase 3 : Migration Cloud (AWS)

### 3.1 Configuration AWS

**Objectif** : Pr√©parer l'environnement cloud conforme RGPD

**Actions** :
- [ ] Cr√©er un compte AWS (si n√©cessaire)
- [ ] Configurer IAM (utilisateur, r√¥les, policies)
- [ ] Cr√©er un bucket S3 dans une r√©gion europ√©enne (eu-west-1, eu-central-1)
- [ ] Configurer les permissions S3
- [ ] Uploader le dataset sur S3

**Livrables** :
- Bucket S3 configur√© et accessible
- Dataset upload√© sur S3
- Documentation des configurations

### 3.2 Configuration EMR

**Objectif** : Cr√©er et configurer le cluster EMR

**Actions** :
- [ ] Suivre la documentation de l'alternant pour cr√©er le cluster
- [ ] S√©lectionner la r√©gion europ√©enne
- [ ] Configurer les instances (type, nombre de nodes)
- [ ] Ajouter les bootstrap actions pour installer les d√©pendances
- [ ] Configurer les security groups
- [ ] Cr√©er la paire de cl√©s SSH

**Configuration recommand√©e** :
- **R√©gion** : eu-west-1 (Irlande) ou eu-central-1 (Francfort)
- **Release** : EMR 7.10.0 (ou 7.x latest)
  - Spark 3.5.5
  - TensorFlow 2.16.1
  - JupyterHub (latest)
  - Hadoop 3.4.1
- **Instances** : 1 master m5.xlarge + 2 core m5.xlarge (ajustable selon budget)
- **Applications** : Spark, JupyterHub, Hadoop, TensorFlow

**‚ö†Ô∏è Changement important (EMR 7.10+)** :
- S3A filesystem remplace EMRFS comme connecteur S3 par d√©faut
- Adapter les chemins S3 si n√©cessaire (g√©n√©ralement transparent)

**Livrables** :
- Cluster EMR op√©rationnel
- Documentation de la configuration

### 3.3 Connexion et setup du notebook

**Objectif** : Acc√©der au notebook cloud et pr√©parer l'environnement

**Actions** :
- [ ] Cr√©er le tunnel SSH vers le master node
- [ ] Configurer FoxyProxy pour acc√©der aux interfaces web
- [ ] Se connecter √† JupyterHub
- [ ] Cr√©er un nouveau notebook PySpark
- [ ] Installer les packages n√©cessaires (TensorFlow, PIL)

**Livrables** :
- Acc√®s JupyterHub fonctionnel
- Notebook PySpark pr√™t

### 3.4 Ex√©cution du pipeline sur EMR

**Objectif** : Ex√©cuter la cha√Æne compl√®te sur le cloud

**Actions** :
- [ ] Copier le code du notebook local vers EMR
- [ ] Adapter les chemins S3 (input/output)
- [ ] Ex√©cuter le pipeline par √©tapes
- [ ] Monitorer l'ex√©cution via Spark UI
- [ ] V√©rifier les outputs sur S3
- [ ] T√©l√©charger et valider les r√©sultats

**Livrables** :
- Pipeline ex√©cut√© avec succ√®s sur EMR
- Donn√©es de sortie (features + PCA) sur S3
- Logs d'ex√©cution et m√©triques

### 3.5 Optimisation et validation

**Objectif** : Optimiser les performances et valider les r√©sultats

**Actions** :
- [ ] Analyser les m√©triques Spark (temps, shuffle, etc.)
- [ ] Ajuster les param√®tres si n√©cessaire
- [ ] V√©rifier la conformit√© RGPD (r√©gion EU)
- [ ] Tester avec le dataset complet
- [ ] Valider la qualit√© des r√©sultats PCA

**Livrables** :
- Rapport de performance
- R√©sultats valid√©s

### 3.6 Arr√™t et nettoyage

**Objectif** : G√©rer les co√ªts et nettoyer les ressources

**Actions** :
- [ ] ‚ö†Ô∏è **IMPORTANT** : Arr√™ter le cluster EMR apr√®s utilisation
- [ ] V√©rifier que toutes les donn√©es sont bien sur S3
- [ ] Optionnel : Cloner la configuration du cluster pour r√©utilisation
- [ ] Documenter les co√ªts r√©els

**Livrables** :
- Cluster arr√™t√©
- Configuration sauvegard√©e pour r√©utilisation

---

## Phase 4 : Documentation et Pr√©sentation

### 4.1 Finalisation du notebook

**Objectif** : Produire un notebook propre et document√©

**Actions** :
- [ ] Nettoyer le code (supprimer les tests, commentaires inutiles)
- [ ] Ajouter des commentaires explicatifs
- [ ] Structurer avec des sections markdown claires
- [ ] Ajouter des visualisations (si pertinent)
- [ ] Exporter en .ipynb et .html

**Livrables** :
- Notebook final propre et document√©
- Export HTML pour visualisation

### 4.2 Organisation des donn√©es S3

**Objectif** : Structurer les donn√©es sur S3 de mani√®re claire

**Structure S3 recommand√©e** :
```
s3://mon-bucket-fruits/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                  # Images originales
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Training/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Test/
‚îÇ   ‚îú‚îÄ‚îÄ features/             # Features extraites (MobileNetV2)
‚îÇ   ‚îî‚îÄ‚îÄ pca/                  # R√©sultats PCA
‚îú‚îÄ‚îÄ models/                   # Weights du mod√®le (optionnel)
‚îî‚îÄ‚îÄ logs/                     # Logs d'ex√©cution (optionnel)
```

**Actions** :
- [ ] Organiser les fichiers sur S3
- [ ] Documenter l'arborescence
- [ ] V√©rifier les permissions d'acc√®s

**Livrables** :
- Arborescence S3 claire et document√©e

### 4.3 Pr√©paration de la pr√©sentation

**Objectif** : Cr√©er le support de pr√©sentation pour la soutenance

**Contenu attendu** (20 minutes) :
1. **Probl√©matique et dataset** (3 min)
   - Contexte "Fruits!"
   - Dataset Fruits-360
   - Objectifs du projet

2. **Architecture Big Data** (6 min)
   - Sch√©ma de l'architecture AWS (EMR + S3)
   - Justification des choix techniques
   - Conformit√© RGPD
   - Gestion des co√ªts

3. **Pipeline PySpark** (6 min)
   - Vue d'ensemble du pipeline
   - Broadcast des weights TensorFlow (explication + code)
   - PCA en PySpark (explication + code)
   - Optimisations r√©alis√©es

4. **D√©monstration** (2 min)
   - Ex√©cution du notebook sur EMR
   - Visualisation des r√©sultats

5. **Conclusion** (3 min)
   - R√©sultats obtenus
   - Retour critique sur la solution
   - Scalabilit√© et perspectives

**Actions** :
- [ ] Cr√©er le support (PowerPoint, Google Slides, etc.)
- [ ] Cr√©er les sch√©mas d'architecture
- [ ] Pr√©parer des screenshots du code cl√©
- [ ] Pr√©parer la d√©monstration live
- [ ] R√©p√©ter la pr√©sentation (timing)

**Livrables** :
- Support de pr√©sentation finalis√©
- Script de d√©monstration

---

## Checklist Finale

### Avant la soutenance

- [ ] Notebook PySpark complet et test√© sur EMR
- [ ] Broadcast TensorFlow impl√©ment√© et fonctionnel
- [ ] PCA en PySpark impl√©ment√©e et valid√©e
- [ ] Donn√©es sur S3 (images + features + PCA)
- [ ] Cluster EMR arr√™t√© (co√ªts ma√Ætris√©s)
- [ ] Pr√©sentation finalis√©e et r√©p√©t√©e
- [ ] D√©monstration test√©e
- [ ] Tous les livrables pr√™ts :
  - `Nom_Pr√©nom_1_notebook_mmaaaa.ipynb`
  - `Nom_Pr√©nom_2_images_mmaaaa` (lien S3 ou export)
  - `Nom_Pr√©nom_3_presentation_mmaaaa.pdf`

### V√©rifications techniques

- [ ] Conformit√© RGPD : serveurs en r√©gion europ√©enne ‚úì
- [ ] Scalabilit√© : code PySpark distribu√© ‚úì
- [ ] Performance : broadcast + optimisations ‚úì
- [ ] Documentation : code comment√© et clair ‚úì
- [ ] R√©sultats : PCA output valid√© ‚úì

---

## Estimation des co√ªts AWS

**EMR Cluster** (configuration recommand√©e) :
- 1 master m5.xlarge + 2 core m5.xlarge
- R√©gion eu-west-1
- Co√ªt estim√© : ~2-3‚Ç¨/heure

**Strat√©gie de gestion des co√ªts** :
1. D√©velopper et tester en local au maximum
2. Utiliser EMR uniquement pour les tests finaux et la d√©mo
3. Arr√™ter le cluster imm√©diatement apr√®s utilisation
4. Pr√©voir 3-4 heures max de cluster actif
5. **Budget total estim√© : 6-10‚Ç¨**

**S3 Storage** :
- Dataset : ~1GB
- Features + PCA : ~500MB
- Co√ªt stockage : < 0.50‚Ç¨/mois (n√©gligeable)

---

## Points d'attention critiques

‚ö†Ô∏è **Gestion des co√ªts EMR**
- Ne JAMAIS laisser le cluster actif inutilement
- V√©rifier que le cluster est bien arr√™t√© apr√®s chaque session

‚ö†Ô∏è **RGPD**
- Toujours utiliser des r√©gions europ√©ennes (eu-west-1, eu-central-1)
- V√©rifier la r√©gion lors de la cr√©ation du bucket S3 et du cluster EMR

‚ö†Ô∏è **Performance**
- Le broadcast des weights est CRITIQUE pour la performance
- Tester avec un subset avant de lancer sur le dataset complet

‚ö†Ô∏è **Soutenance**
- La d√©monstration doit √™tre fluide et rapide (2 min)
- Avoir un plan B si probl√®me de connexion (screenshots/vid√©o)

---

## Ressources et Documentation

**Documentation officielle** :
- [PySpark ML - PCA](https://spark.apache.org/docs/latest/ml-features.html#pca)
- [PySpark - Broadcast Variables](https://spark.apache.org/docs/latest/rdd-programming-guide.html#broadcast-variables)
- [AWS EMR - Getting Started](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html)
- [TensorFlow + Spark](https://www.tensorflow.org/guide/distributed_training)

**Notebooks et exemples** :
- Notebook de l'alternant : `notebooks/alternant/P8_Notebook_Linux_EMR_PySpark_V1.0.ipynb`
- Mode op√©ratoire : https://s3.eu-west-1.amazonaws.com/course.oc-static.com/projects/Data_Scientist_P8/P8_Mode_ope%CC%81ratoire.zip

---

**Derni√®re mise √† jour** : 24 Octobre 2025
