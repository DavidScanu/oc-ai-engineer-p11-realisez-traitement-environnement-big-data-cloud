# Comprendre les Fichiers GÃ©nÃ©rÃ©s par PySpark

## ðŸ“Š Vue d'ensemble

PySpark gÃ©nÃ¨re des noms de fichiers qui peuvent sembler inhabituels, mais cette structure est parfaitement normale et suit les conventions Apache Spark pour le traitement distribuÃ©.

---

## ðŸ“ Structure des Dossiers du Projet

### **data/features/mobilenetv2_features/**

**Format** : Parquet (format distribuÃ© Apache)

| Fichier | Type | Taille | Explication |
|---------|------|--------|-------------|
| `_SUCCESS` | Marqueur | 0 B | âœ… Indique que l'Ã©criture s'est terminÃ©e avec succÃ¨s |
| `._SUCCESS.crc` | Checksum | 8 B | Fichier de vÃ©rification d'intÃ©gritÃ© |
| `part-00000-*.snappy.parquet` | DonnÃ©es | ~918 KB | ðŸŽ¯ **Fichier principal** contenant les features MobileNetV2 (1280 dimensions) |
| `.part-00000-*.crc` | Checksum | ~7.2 KB | VÃ©rification d'intÃ©gritÃ© du fichier parquet |

---

### **data/pca/pca_results/**

**Format** : Parquet (recommandÃ© pour PySpark)

| Fichier | Type | Taille | Explication |
|---------|------|--------|-------------|
| `_SUCCESS` | Marqueur | 0 B | âœ… Ã‰criture rÃ©ussie |
| `._SUCCESS.crc` | Checksum | 8 B | VÃ©rification d'intÃ©gritÃ© |
| `part-00000-*.snappy.parquet` | DonnÃ©es | ~139 KB | ðŸŽ¯ **RÃ©sultats PCA** (images Ã— 200 dimensions) |
| `.part-00000-*.crc` | Checksum | ~1.1 KB | VÃ©rification d'intÃ©gritÃ© |

**Contenu** : Vecteurs PCA denses (format natif PySpark)

---

### **data/pca/pca_results_csv/**

**Format** : CSV (pour inspection humaine)

| Fichier | Type | Taille | Explication |
|---------|------|--------|-------------|
| `_SUCCESS` | Marqueur | 0 B | âœ… Ã‰criture rÃ©ussie |
| `._SUCCESS.crc` | Checksum | 8 B | VÃ©rification d'intÃ©gritÃ© |
| `part-00000-*.csv` | DonnÃ©es | ~407 KB | ðŸŽ¯ **CSV avec header** (path, label, pca_features_string) |
| `.part-00000-*.crc` | Checksum | ~3.2 KB | VÃ©rification d'intÃ©gritÃ© |

**Contenu** : Features PCA au format string (valeurs sÃ©parÃ©es par virgules)

**Colonnes** :
- `path` : Chemin complet vers l'image source
- `label` : Classe de l'image (ex: "Apple Golden 1")
- `pca_features_string` : 200 valeurs PCA sÃ©parÃ©es par virgules

---

## ðŸ” DÃ©cryptage du Format de Nommage

### Anatomie d'un Nom de Fichier PySpark

```
part-00000-4338336c-81f1-4258-9db4-82f13649b008-c000.snappy.parquet
 â”‚     â”‚                    â”‚                      â”‚      â”‚        â”‚
 â”‚     â”‚                    â”‚                      â”‚      â”‚        â””â”€ Extension (.parquet, .csv)
 â”‚     â”‚                    â”‚                      â”‚      â””â”€ Compression (snappy, gzip, none)
 â”‚     â”‚                    â”‚                      â””â”€ Partition chunk ID (c000, c001, etc.)
 â”‚     â”‚                    â””â”€ UUID unique du job Spark
 â”‚     â””â”€ NumÃ©ro de partition (00000 = premiÃ¨re partition)
 â””â”€ PrÃ©fixe standard PySpark
```

### Pourquoi cette structure ?

PySpark gÃ©nÃ¨re ces noms pour plusieurs raisons importantes :

1. **Distribution des donnÃ©es** ðŸŒ
   - Les donnÃ©es sont divisÃ©es en partitions pour le traitement parallÃ¨le
   - Chaque partition = 1 fichier `part-XXXXX`
   - Permet de traiter des tÃ©raoctets de donnÃ©es

2. **UnicitÃ© et traÃ§abilitÃ©** ðŸ”
   - UUID unique Ã©vite les collisions de fichiers
   - Permet de tracer quel job Spark a crÃ©Ã© le fichier
   - Essentiel pour les environnements multi-utilisateurs (AWS EMR)

3. **Optimisation** âš¡
   - Compression Snappy pour gain d'espace (~3-5x)
   - Format columnaire Parquet pour lectures rapides
   - Chunks permettent le streaming de gros datasets

---

## ðŸ“– Fichiers SpÃ©ciaux

### `_SUCCESS`

- **RÃ´le** : Marqueur de succÃ¨s
- **Taille** : 0 bytes (fichier vide)
- **Signification** : L'opÃ©ration d'Ã©criture s'est terminÃ©e **sans erreur**
- **Important** : Si ce fichier est absent â†’ l'Ã©criture a Ã©chouÃ© ou est incomplÃ¨te

### Fichiers `.crc`

- **RÃ´le** : Checksum CRC32
- **But** : VÃ©rifier l'intÃ©gritÃ© des donnÃ©es
- **UtilisÃ© par** : Hadoop/Spark pour dÃ©tecter les corruptions
- **Note** : Peuvent Ãªtre ignorÃ©s pour l'utilisation manuelle

---

## ðŸ’» Utilisation des Fichiers

### Recharger en PySpark

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Reload").getOrCreate()

# Charger les features MobileNetV2
df_features = spark.read.parquet("data/features/mobilenetv2_features")
print(f"Features chargÃ©es: {df_features.count()} images")

# Charger les rÃ©sultats PCA (format Parquet)
df_pca = spark.read.parquet("data/pca/pca_results")
print(f"PCA chargÃ©es: {df_pca.count()} images")

# Charger les rÃ©sultats PCA (format CSV)
df_pca_csv = spark.read.csv("data/pca/pca_results_csv", header=True)
print(f"PCA CSV chargÃ©es: {df_pca_csv.count()} images")

# Afficher le schÃ©ma
df_pca.printSchema()
df_pca.show(5)
```

### Lire avec Pandas

```python
import pandas as pd
import glob

# Lire le CSV (plus simple pour pandas)
csv_files = glob.glob("data/pca/pca_results_csv/part-*.csv")
df = pd.read_csv(csv_files[0])

print(f"Shape: {df.shape}")
print(df.head())

# Convertir la colonne pca_features_string en array numpy
import numpy as np

def string_to_array(s):
    return np.array([float(x) for x in s.split(',')])

df['pca_features'] = df['pca_features_string'].apply(string_to_array)
```

### Lire avec PyArrow (recommandÃ© pour Parquet)

```python
import pyarrow.parquet as pq
import pandas as pd

# Lire un fichier Parquet avec PyArrow
table = pq.read_table("data/pca/pca_results")
df = table.to_pandas()

print(df.head())
```

---

## ðŸš€ Migration vers AWS S3

### DiffÃ©rences Local vs Cloud

| Aspect | Local | AWS EMR + S3 |
|--------|-------|--------------|
| **Chemin** | `file:///path/to/data` | `s3://bucket/path/` |
| **Partitions** | 1-2 fichiers | Dizaines/centaines selon cluster |
| **Compression** | Optionnelle | **RecommandÃ©e** (coÃ»t S3) |
| **`_SUCCESS`** | Important | **Critique** (indicateur de succÃ¨s) |

### Exemple de Chemins S3

```python
# Lecture depuis S3
df = spark.read.parquet("s3://mon-bucket-fruits/data/features/mobilenetv2_features/")

# Ã‰criture vers S3
df_pca.write.mode("overwrite").parquet("s3://mon-bucket-fruits/data/pca/pca_results/")

# Ã‰criture CSV vers S3
df_pca_csv.write.mode("overwrite") \
    .option("header", "true") \
    .csv("s3://mon-bucket-fruits/data/pca/pca_results_csv/")
```

### Bonnes Pratiques S3

1. **Toujours utiliser Parquet** sur S3 (plus Ã©conomique que CSV)
2. **Activer la compression** (Snappy par dÃ©faut)
3. **VÃ©rifier `_SUCCESS`** aprÃ¨s chaque Ã©criture
4. **Partitionner intelligemment** pour gros datasets :
   ```python
   df.write.partitionBy("label").parquet("s3://bucket/data/")
   ```

---

## ðŸ”§ Gestion des Fichiers

### Lister tous les fichiers d'un dataset

```bash
# Local
ls -lah data/pca/pca_results/

# AWS S3
aws s3 ls s3://mon-bucket-fruits/data/pca/pca_results/
```

### Compter les partitions

```python
import os
import glob

# Compter les fichiers part-*
parquet_files = glob.glob("data/pca/pca_results/part-*.parquet")
print(f"Nombre de partitions: {len(parquet_files)}")

# En PySpark
df = spark.read.parquet("data/pca/pca_results")
print(f"Nombre de partitions: {df.rdd.getNumPartitions()}")
```

### Fusionner les partitions (coalesce)

```python
# RÃ©duire le nombre de partitions (utile pour petits datasets)
df_pca.coalesce(1).write.mode("overwrite").parquet("data/pca/pca_single_file/")

# VÃ©rification
# âš ï¸ Attention: coalesce(1) crÃ©e un seul fichier (peut Ãªtre lent pour gros datasets)
```

---

## âš ï¸ ProblÃ¨mes Courants

### ProblÃ¨me 1 : Fichier `_SUCCESS` manquant

**Cause** : L'Ã©criture a Ã©chouÃ© ou Ã©tÃ© interrompue

**Solution** :
```python
# VÃ©rifier l'existence de _SUCCESS
import os
if os.path.exists("data/pca/pca_results/_SUCCESS"):
    print("âœ… Ã‰criture rÃ©ussie")
else:
    print("âŒ Ã‰criture incomplÃ¨te - relancer le job")
```

### ProblÃ¨me 2 : Trop de petits fichiers

**Cause** : Trop de partitions pour un petit dataset

**Solution** :
```python
# Repartitionner avant sauvegarde
df.coalesce(4).write.parquet("data/output/")  # 4 fichiers au lieu de 200
```

### ProblÃ¨me 3 : Fichiers CRC causent des erreurs

**Cause** : Certains outils ne supportent pas les fichiers `.crc`

**Solution** :
```bash
# Supprimer les fichiers .crc (optionnel, ils se rÃ©gÃ©nÃ¨rent)
find data/pca/ -name "*.crc" -delete
```

### ProblÃ¨me 4 : Impossible de lire avec pandas

**Cause** : Pandas ne supporte pas nativement les dossiers Parquet

**Solution** :
```python
# Option 1: Utiliser PyArrow
import pyarrow.parquet as pq
table = pq.read_table("data/pca/pca_results")
df = table.to_pandas()

# Option 2: Lire un fichier spÃ©cifique
import glob
parquet_file = glob.glob("data/pca/pca_results/part-*.parquet")[0]
df = pd.read_parquet(parquet_file)
```

---

## ðŸ“Š Comparaison des Formats

### Parquet vs CSV

| CritÃ¨re | Parquet | CSV |
|---------|---------|-----|
| **Taille** | ðŸŸ¢ CompressÃ© (3-10x plus petit) | ðŸ”´ Non compressÃ© |
| **Vitesse lecture** | ðŸŸ¢ TrÃ¨s rapide (columnaire) | ðŸ”´ Lent (ligne par ligne) |
| **CompatibilitÃ©** | ðŸŸ¡ NÃ©cessite PyArrow/Spark | ðŸŸ¢ Universel |
| **Types de donnÃ©es** | ðŸŸ¢ PrÃ©servÃ©s (int, float, vector) | ðŸ”´ Tout en string |
| **Inspection humaine** | ðŸ”´ Binaire (illisible) | ðŸŸ¢ Texte lisible |
| **Recommandation** | â­ Production/Cloud | ðŸ“ Debug/Inspection |

### Quand utiliser quoi ?

- **Parquet** :
  - âœ… Production sur AWS EMR/S3
  - âœ… Gros datasets (>1 GB)
  - âœ… RÃ©utilisation dans PySpark
  - âœ… Performance critique

- **CSV** :
  - âœ… Inspection manuelle
  - âœ… Partage avec non-data scientists
  - âœ… Import dans Excel/Google Sheets
  - âœ… Petit datasets (<100 MB)

---

## ðŸŽ¯ Checklist de Validation

AprÃ¨s chaque opÃ©ration PySpark d'Ã©criture, vÃ©rifier :

- [ ] Fichier `_SUCCESS` prÃ©sent
- [ ] Au moins un fichier `part-*.{parquet|csv}` existe
- [ ] Taille des fichiers cohÃ©rente (pas de 0 bytes)
- [ ] PossibilitÃ© de recharger les donnÃ©es avec `spark.read`
- [ ] Nombre de lignes cohÃ©rent avec le dataset source

```python
# Script de validation automatique
import os
import glob

def validate_pyspark_output(path):
    """Valide qu'un dossier PySpark est correct."""

    # VÃ©rifier _SUCCESS
    success_file = os.path.join(path, "_SUCCESS")
    if not os.path.exists(success_file):
        print(f"âŒ Fichier _SUCCESS manquant dans {path}")
        return False

    # VÃ©rifier prÃ©sence de fichiers part-*
    part_files = glob.glob(os.path.join(path, "part-*"))
    if len(part_files) == 0:
        print(f"âŒ Aucun fichier part-* trouvÃ© dans {path}")
        return False

    # VÃ©rifier que les fichiers ne sont pas vides
    for f in part_files:
        if os.path.getsize(f) == 0:
            print(f"âŒ Fichier vide: {f}")
            return False

    print(f"âœ… {path} est valide ({len(part_files)} partitions)")
    return True

# Utilisation
validate_pyspark_output("data/pca/pca_results")
validate_pyspark_output("data/features/mobilenetv2_features")
```

---

## ðŸ“š Ressources

- [Apache Parquet Documentation](https://parquet.apache.org/docs/)
- [PySpark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- [AWS S3 Best Practices](https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html)
- [Snappy Compression](https://github.com/google/snappy)

---

## ðŸ’¡ RÃ©sumÃ©

**Les noms de fichiers PySpark sont normaux et suivent les conventions Spark !**

- âœ… Structure conforme aux standards Apache Spark
- âœ… Fichiers `_SUCCESS` = indicateur de succÃ¨s critique
- âœ… UUID dans les noms = unicitÃ© garantie
- âœ… Format Parquet + Snappy = optimal pour production
- âœ… CSV disponible pour inspection manuelle

**Ne pas renommer manuellement ces fichiers** - PySpark s'attend Ã  cette structure exacte pour les lectures futures.