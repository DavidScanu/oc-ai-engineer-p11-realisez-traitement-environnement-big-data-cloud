# Projet Big Data & Cloud Computing - Fruits!

## üìã Contexte du Projet

### Pr√©sentation de l'entreprise
Vous √™tes Data Scientist dans **"Fruits!"**, une jeune start-up de l'AgriTech qui cherche √† proposer des solutions innovantes pour la r√©colte des fruits.

### Objectif de l'entreprise
- **Mission principale** : Pr√©server la biodiversit√© des fruits en permettant des traitements sp√©cifiques pour chaque esp√®ce
- **Solution** : D√©velopper des robots cueilleurs intelligents

### Application mobile
La start-up souhaite se faire conna√Ætre en mettant √† disposition du grand public une application mobile permettant :
- De prendre en photo un fruit
- D'obtenir des informations sur ce fruit
- De sensibiliser le grand public √† la biodiversit√© des fruits
- De mettre en place une premi√®re version du moteur de classification des images de fruits

---

## üéØ Votre Mission

### Point de d√©part
Votre coll√®gue **Paul** vous indique l'existence d'un document formalis√© par un alternant qui vient de quitter l'entreprise. Il a test√© une premi√®re approche dans un environnement Big Data AWS EMR.

**Votre r√¥le** : Vous approprier les travaux r√©alis√©s par l'alternant et compl√©ter la cha√Æne de traitement.

> ‚ö†Ô∏è **Important** : Il n'est pas n√©cessaire d'entra√Æner un mod√®le pour le moment. L'important est de mettre en place les premi√®res briques de traitement qui serviront lorsqu'il faudra passer √† l'√©chelle en termes de volume de donn√©es !

### Contraintes et points d'attention

#### 1. **Scalabilit√©**
- Le volume de donn√©es va augmenter tr√®s rapidement apr√®s la livraison du projet
- D√©velopper des scripts en **PySpark**
- Utiliser le cloud **AWS** pour profiter d'une architecture Big Data (EMR, S3, IAM)
- Alternative possible : **Databricks**

#### 2. **D√©monstration requise**
Vous devez faire une d√©monstration de :
- La mise en place d'une instance EMR op√©rationnelle
- L'explication pas √† pas du script PySpark compl√©t√© avec :
  - **Traitement de diffusion des poids** du mod√®le Tensorflow sur les clusters (broadcast des "weights" du mod√®le) - oubli√© par l'alternant
  - **√âtape de r√©duction de dimension** de type PCA en PySpark

#### 3. **RGPD**
‚ö° **Contrainte importante** : Param√©trer votre installation afin d'utiliser des serveurs situ√©s sur le **territoire europ√©en**

#### 4. **Retour critique**
Votre retour critique de cette solution sera pr√©cieux avant de d√©cider de la g√©n√©raliser

#### 5. **Gestion des co√ªts**
- La mise en ≈ìuvre d'une architecture Big Data de type EMR engendrera des co√ªts
- **Co√ªt estim√©** : < 10 euros pour une utilisation raisonn√©e (√† votre charge)
- ‚ö†Ô∏è **Attention** : Ne pas laisser le cluster EMR ouvert lorsque vous ne l'utilisez pas
- **Recommandation** : Utiliser un serveur local pour la mise √† jour du Script PySpark, en limitant l'utilisation du serveur EMR √† l'impl√©mentation et aux tests

---

## üìö Ressources

### Jeu de donn√©es
- **Kaggle** : https://www.kaggle.com/datasets/moltean/fruits
- **T√©l√©chargement direct** : https://s3.eu-west-1.amazonaws.com/course.oc-static.com/projects/Data_Scientist_P8/fruits.zip

### Notebook de l'alternant
- **Mode op√©ratoire** : https://s3.eu-west-1.amazonaws.com/course.oc-static.com/projects/Data_Scientist_P8/P8_Mode_ope%CC%81ratoire.zip

### Article de r√©f√©rence
- **Model inference using TensorFlow Keras API** (en anglais) - disponible dans les ressources

---

## üõ†Ô∏è √âtapes de R√©alisation

### √âtape 1 : Pr√©parer la cha√Æne de traitement PySpark en local
- Reprendre le notebook de l'alternant
- Compl√©ter la d√©marche, notamment avec une √©tape de r√©duction de dimension en PySpark

### √âtape 2 : Migrer votre cha√Æne de traitement dans le Cloud (AWS)
- Prendre connaissance des services AWS
- Identifier les services pertinents pour migrer chaque √©tape de votre cha√Æne de traitement
- Mettre en place le cluster EMR pour distribuer les calculs dans le cloud
- Connecter votre notebook Cloud pour r√©aliser la cha√Æne de traitement jusqu'√† la r√©duction de dimensions

### √âtape 3 : V√©rifier votre travail et pr√©parer la soutenance
- V√©rifier que votre notebook PySpark est clair et les scripts bien comment√©s
- S'assurer que les donn√©es et r√©sultats sont organis√©s et accessibles dans le Cloud
- Pr√©parer une pr√©sentation concise expliquant votre architecture cloud et le processus de traitement PySpark
- Pratiquer la pr√©sentation pour garantir une explication fluide et une d√©monstration technique sans accroc

---

## üì¶ Livrables

1. **Un notebook sur le cloud (Colab)** contenant :
   - Les scripts en PySpark ex√©cutables
   - Le preprocessing
   - Une √©tape de r√©duction de dimension de type PCA

2. **Les donn√©es stock√©es dans le cloud** :
   - Les images du jeu de donn√©es initial
   - La sortie de la r√©duction de dimension (matrice CSV ou autre)
   - Disponibles dans un espace de stockage sur le cloud

3. **Un support de pr√©sentation** pr√©sentant :
   - Les diff√©rentes briques d'architecture choisies sur le cloud
   - Leur r√¥le dans l'architecture Big Data
   - La d√©marche de mise en ≈ìuvre de l'environnement Big Data (EMR ou Databricks)
   - Les √©tapes de la cha√Æne de traitement PySpark

### Convention de nommage
D√©posez dans un dossier zip nomm√© `Titre_du_projet_nom_pr√©nom` :

- `Nom_Pr√©nom_1_notebook_mmaaaa`
- `Nom_Pr√©nom_2_images_mmaaaa`
- `Nom_Pr√©nom_3_presentation_mmaaaa`

**Exemple** : `Dupont_Jean_1_notebook_012024`

---

## üé§ Soutenance (30 minutes)

### Pr√©sentation (20 minutes)
1. **Rappel de la probl√©matique et pr√©sentation du jeu de donn√©es** (3 min)
2. **Pr√©sentation du processus de cr√©ation de l'environnement Big Data, S3 et EMR ou Databricks** (6 min)
3. **Pr√©sentation de la r√©alisation de la cha√Æne de traitement des images dans un environnement Big Data dans le cloud** (6 min)
4. **D√©monstration d'ex√©cution du script PySpark sur le Cloud** (2 min)
5. **Synth√®se et conclusion** (3 min)

### Discussion (5 minutes)
L'√©valuateur (Paul) vous challengera sur votre compr√©hension des concepts et techniques mis en ≈ìuvre

### D√©briefing (5 minutes)
D√©briefing ensemble √† la fin de la soutenance

> ‚ö†Ô∏è **Important** : La pr√©sentation doit durer 20 minutes (+/- 5 minutes). Les pr√©sentations en dessous de 15 minutes ou au-dessus de 25 minutes peuvent √™tre refus√©es.

---

## üéì Comp√©tences √âvalu√©es

### Comp√©tence 1 : S√©lectionner les outils du Cloud
**Objectif** : Traiter et stocker les donn√©es d'un projet Big Data conforme aux normes RGPD

**Crit√®res d'√©valuation** :
- CE1 : Identifier les diff√©rentes briques d'architecture n√©cessaires pour la mise en place d'un environnement Big Data
- CE2 : Identifier les outils du cloud permettant de mettre en place l'environnement Big Data conforme aux normes RGPD

### Comp√©tence 2 : Pr√©traiter, analyser et mod√©liser
**Objectif** : Traiter des donn√©es dans un environnement Big Data en utilisant les outils du Cloud

**Crit√®res d'√©valuation** :
- CE1 : Charger les fichiers de d√©part et ceux apr√®s transformation dans un espace de stockage cloud conforme √† la r√©glementation RGPD
- CE2 : Ex√©cuter les scripts en utilisant des machines dans le cloud
- CE3 : R√©aliser un script qui permet d'√©crire les sorties du programme directement dans l'espace de stockage cloud

### Comp√©tence 3 : R√©aliser des calculs distribu√©s
**Objectif** : Traiter des donn√©es massives en utilisant les outils adapt√©s et en prenant en compte le RGPD

**Crit√®res d'√©valuation** :
- CE1 : Identifier les traitements critiques lors d'un passage √† l'√©chelle en termes de volume de donn√©es
- CE2 : Veiller √† ce que l'exploitation des donn√©es soit conforme au RGPD (serveurs europ√©ens)
- CE3 : D√©velopper les scripts s'appuyant sur Spark
- CE4 : S'assurer que toute la cha√Æne de traitement est ex√©cut√©e dans le cloud

---

## üéØ Ce que vous allez apprendre

### Big Data et Cloud Computing
- Traitement de donn√©es massives
- Migration d'un projet Data de l'environnement local vers un environnement Big Data
- Utilisation de **PySpark**
- Travail dans des environnements Cloud distribu√©s avec services pr√™t-√†-l'emploi
- Gestion de grands volumes de donn√©es
- Conception d'une architecture Big Data garantissant l'efficacit√© et la scalabilit√©

### Pourquoi ces comp√©tences sont importantes ?
Dans un monde o√π le volume de donn√©es ne cesse de cro√Ætre, la capacit√© √† travailler avec le Big Data est essentielle. Ces comp√©tences :
- Ouvrent des portes dans de nombreux secteurs (finance, sant√©, marketing, etc.)
- Permettent de d√©velopper des solutions innovantes
- Vous positionnent comme un acteur cl√© capable de relever des d√©fis modernes en data
- Permettent d'apporter des insights pr√©cieux √† partir de grandes quantit√©s de donn√©es

---

## ‚úÖ Checklist du Projet

- [ ] T√©l√©charger et explorer le jeu de donn√©es Fruits
- [ ] T√©l√©charger et analyser le notebook de l'alternant
- [ ] Compl√©ter le notebook avec la r√©duction de dimension PCA en PySpark
- [ ] Ajouter le broadcast des weights du mod√®le TensorFlow
- [ ] Cr√©er un compte AWS (r√©gion europ√©enne)
- [ ] Configurer S3 pour le stockage des donn√©es
- [ ] Mettre en place un cluster EMR
- [ ] Tester la cha√Æne de traitement sur le cloud
- [ ] Pr√©parer le support de pr√©sentation
- [ ] V√©rifier la conformit√© RGPD (serveurs europ√©ens)
- [ ] Pr√©parer la d√©monstration
- [ ] ‚ö†Ô∏è Arr√™ter le cluster EMR apr√®s utilisation

---

**Bonne chance ! üöÄ**